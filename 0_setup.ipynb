{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import glob\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s,%(module)s,%(processName)s,%(levelname)s,%(message)s', level=logging.INFO, stream=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:08:31,533,2625127137,MainProcess,INFO,config read from config.yaml -> {\n",
      "  \"app_name\": \"contact-center-transcript-summarization\",\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"Admin\"\n",
      "  },\n",
      "  \"dir\": {\n",
      "    \"data\": \"data\",\n",
      "    \"raw\": \"data/raw\",\n",
      "    \"golden\": \"data/raw/golden\",\n",
      "    \"prompts\": \"data/prompts\",\n",
      "    \"models\": \"data/models\",\n",
      "    \"metrics\": \"data/metrics\",\n",
      "    \"completions\": \"data/completions\"\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"raw_data_file\": \"data.csv\",\n",
      "    \"golden_transcript\": \"data/raw/golden/transcript.txt\",\n",
      "    \"golden_transcript_summary\": \"data/raw/golden/summary.txt\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"very_large_prompt\": {\n",
      "      \"sleep_time\": 180,\n",
      "      \"threshold\": 70000\n",
      "    },\n",
      "    \"normal_prompt\": {\n",
      "      \"sleep_time\": 60\n",
      "    }\n",
      "  },\n",
      "  \"max_retries\": 3,\n",
      "  \"desired_word_count_for_summary\": 80,\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"single-line-reason\",\n",
      "      \"prompt_template\": null,\n",
      "      \"reps\": 3,\n",
      "      \"model_list\": [\n",
      "        {\n",
      "          \"model\": \"anthropic.claude-instant-v1\",\n",
      "          \"prompt_template\": \"anthropic_template_single_line_reason.txt\"\n",
      "        },\n",
      "        {\n",
      "          \"model\": \"amazon.titan-text-express-v1\",\n",
      "          \"prompt_template\": \"titan_template.txt\"\n",
      "        },\n",
      "        {\n",
      "          \"model\": \"cohere.command-text-v14\",\n",
      "          \"prompt_template\": \"cohere_template.txt\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"bedrock_models\": {\n",
      "    \"cohere.command-text-v14\": {\n",
      "      \"context_length\": 4000,\n",
      "      \"prompt_token_pricing_per_million\": 1.5,\n",
      "      \"completion_token_pricing_per_million\": 2.0,\n",
      "      \"inference_param_set\": \"cohere\"\n",
      "    },\n",
      "    \"amazon.titan-text-express-v1\": {\n",
      "      \"context_length\": 8000,\n",
      "      \"prompt_token_pricing_per_million\": 0.8,\n",
      "      \"completion_token_pricing_per_million\": 1.6,\n",
      "      \"inference_param_set\": \"titan\"\n",
      "    },\n",
      "    \"anthropic.claude-v2:1\": {\n",
      "      \"context_length\": 100000,\n",
      "      \"prompt_token_pricing_per_million\": 8,\n",
      "      \"completion_token_pricing_per_million\": 24,\n",
      "      \"inference_param_set\": \"claude\"\n",
      "    },\n",
      "    \"anthropic.claude-instant-v1\": {\n",
      "      \"context_length\": 9000,\n",
      "      \"prompt_token_pricing_per_million\": 1.63,\n",
      "      \"completion_token_pricing_per_million\": 5.51,\n",
      "      \"inference_param_set\": \"claude\"\n",
      "    }\n",
      "  },\n",
      "  \"inference_params\": {\n",
      "    \"claude\": {\n",
      "      \"max_tokens_to_sample\": 512,\n",
      "      \"temperature\": 0.1,\n",
      "      \"top_k\": 250,\n",
      "      \"top_p\": 0.5,\n",
      "      \"stop_sequences\": [],\n",
      "      \"anthropic_version\": \"bedrock-2023-05-31\"\n",
      "    },\n",
      "    \"titan\": {\n",
      "      \"maxTokenCount\": 512,\n",
      "      \"stopSequences\": [],\n",
      "      \"temperature\": 0.1,\n",
      "      \"topP\": 0.9\n",
      "    },\n",
      "    \"cohere\": {\n",
      "      \"max_tokens\": 512,\n",
      "      \"temperature\": 0.1,\n",
      "      \"p\": 0.9,\n",
      "      \"k\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:08:31,540,3397252753,MainProcess,INFO,there are 5 files to read ['data/raw/call_center_transcript_1.txt', 'data/raw/call_center_transcript_0.txt', 'data/raw/call_center_transcript_2.txt', 'data/raw/call_center_transcript_3.txt', 'data/raw/call_center_transcript_4.txt']\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob(os.path.join(config['dir']['raw'], \"*.txt\"))\n",
    "file_list = [f for f in file_list if 'labels.txt' not in f]\n",
    "# exclude labels.txt\n",
    "\n",
    "logger.info(f\"there are {len(file_list)} files to read {file_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'call_center_transcript_0.txt': '<output> Here are the action items I gathered for each person: A: - Document any other user entry concerns to provide to B B: - Work with James from another team to simplify additional forms and sign up workflow C:  - Work on landing page to make product more discoverable </output>',\n",
       " 'call_center_transcript_1.txt': 'Here are the action items I gathered for each person from the conversation: <output> A: - Set up a follow-up meeting to brainstorm ideas for where generative AI could be applicable in our products - Outline high-level ideas for where generative AI could drive automation or enhance user experience in our products B: - Research current generative AI initiatives at other tech companies to analyze the competitive landscape C: - Develop a validation framework to rigorously assess accuracy, ethics and safety of any generative AI applications we develop </output>',\n",
       " 'call_center_transcript_2.txt': 'Here are the action items I gathered for each person from the conversation: <output> A:   - Research recent trends and growth in mobile gaming to identify opportunities - Define process for rapidly prototyping and testing game concepts B: - Outline pros and cons for freemium vs paid monetization models  - Evaluate AI-powered game prototyping tools to complement our development efforts C: - Explore in-game ad networks and pricing - Brainstorm ideas to maximize user retention - Ensure quality bar for any games we officially launch </output>',\n",
       " 'call_center_transcript_3.txt': 'Here are the action items I gathered for each person from the conversation: <output> A:   - Explore methodologies for value alignment in AI systems B: - Draft a proposal for an AI model safety review framework - Research existing XAI (explainable AI) techniques to augment our models C:  - Outline additional potential safety risks to address such as data/algorithm bias and security - Develop testing and monitoring standards for AI models based on industry best practices </output>',\n",
       " 'call_center_transcript_4.txt': 'Here are the action items I gathered for each person from the conversation: <output> A: - Benchmark performance of recommendation model against different GPU instance types (NVIDIA T4, A100, etc.) - Explore using Elastic Inference GPU attachments for cost savings  B: - None explicitly stated C:  - Explore optimizing the specific type of GPU instance to find the best balance of throughput versus cost - Test compatibility of Elastic Inference GPU attachments with model optimizations  </output>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the labels.txt which has the golden summary\n",
    "fpath = os.path.join(os.path.join(config['dir']['raw'], \"labels.txt\"))\n",
    "with open(fpath) as f:\n",
    "    labels = f.readlines()\n",
    "\n",
    "summary = dict()\n",
    "for l in labels:\n",
    "    if l == '\\n':\n",
    "        continue\n",
    "    tokens = l.split('|')\n",
    "    #summary.append(dict(fname=f\"{tokens[0].strip()}.txt\", problems=tokens[1]))\n",
    "    k = f\"{tokens[0].strip()}.txt\"\n",
    "    v = tokens[1].strip()\n",
    "    summary[k] = v\n",
    "summary                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:08:31,552,3809902715,MainProcess,INFO,file number=1, file=data/raw/call_center_transcript_1.txt\n",
      "2024-01-04 18:08:31,554,3809902715,MainProcess,INFO,file number=2, file=data/raw/call_center_transcript_0.txt\n",
      "2024-01-04 18:08:31,556,3809902715,MainProcess,INFO,file number=3, file=data/raw/call_center_transcript_2.txt\n",
      "2024-01-04 18:08:31,558,3809902715,MainProcess,INFO,file number=4, file=data/raw/call_center_transcript_3.txt\n",
      "2024-01-04 18:08:31,561,3809902715,MainProcess,INFO,file number=5, file=data/raw/call_center_transcript_4.txt\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(file_list):\n",
    "    logger.info(f\"file number={i+1}, file={f}\")\n",
    "    dir_path = os.path.join(config['dir']['raw'], str(i))\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    fname = os.path.basename(f)\n",
    "    fpath = os.path.join(dir_path, f\"{fname.replace('.txt', '')}_transcript.txt\")\n",
    "    Path(fpath).write_text(Path(f).read_text())\n",
    "\n",
    "    # find the golden summary corresponding to this file\n",
    "    golden_summary = summary.get(fname)\n",
    "    if golden_summary is None:\n",
    "        logger.error(f\"no summary found for {fname}\")\n",
    "    else:\n",
    "        fpath = os.path.join(dir_path, f\"{fname.replace('.txt', '')}_golden_summary.txt\")\n",
    "        Path(fpath).write_text(golden_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
