{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle asynchronous calls for Bedrock Models with concurrent calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import glob\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "from tokenizer_utils import count_tokens\n",
    "from bedrock_utils import get_bedrock_client\n",
    "from utils import  (\n",
    "    get_rouge_l_score,\n",
    "    get_cosine_similarity,\n",
    "    parse_model_response,\n",
    "    is_amazon_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the logger to log all of the information in\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the config file to get all global constants\n",
    "CONFIG_FILE_PATH = \"config.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-12 19:35:57,781] p47653 {3034282685.py:5} INFO - config read from config.yaml -> {\n",
      "  \"app_name\": \"contact-center-transcript-summarization\",\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"Admin\"\n",
      "  },\n",
      "  \"dir\": {\n",
      "    \"data\": \"data\",\n",
      "    \"raw\": \"data/raw\",\n",
      "    \"golden\": \"data/raw/golden\",\n",
      "    \"prompts\": \"data/prompts\",\n",
      "    \"models\": \"data/models\",\n",
      "    \"metrics\": \"data/metrics\",\n",
      "    \"completions\": \"data/completions\",\n",
      "    \"async_completions\": \"data/async_completions\"\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"raw_data_file\": \"data.csv\",\n",
      "    \"golden_transcript\": \"data/raw/golden/transcript.txt\",\n",
      "    \"golden_transcript_summary\": \"data/raw/golden/summary.txt\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"very_large_prompt\": {\n",
      "      \"sleep_time\": 180,\n",
      "      \"threshold\": 70000\n",
      "    },\n",
      "    \"normal_prompt\": {\n",
      "      \"sleep_time\": 60\n",
      "    }\n",
      "  },\n",
      "  \"max_retries\": 3,\n",
      "  \"desired_word_count_for_summary\": 80,\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"single-line-reason\",\n",
      "      \"prompt_template\": null,\n",
      "      \"reps\": 3,\n",
      "      \"model_list\": [\n",
      "        {\n",
      "          \"model\": \"amazon.titan-text-express-v1\",\n",
      "          \"prompt_template\": \"titan_template.txt\",\n",
      "          \"concurrency_metric\": [\n",
      "            5\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"model\": \"anthropic.claude-instant-v1\",\n",
      "          \"prompt_template\": \"anthropic_template_single_line_reason.txt\",\n",
      "          \"concurrency_metric\": [\n",
      "            5\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"model\": \"cohere.command-text-v14\",\n",
      "          \"prompt_template\": \"cohere_template.txt\",\n",
      "          \"concurrency_metric\": [\n",
      "            5\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"bedrock_models\": {\n",
      "    \"cohere.command-text-v14\": {\n",
      "      \"context_length\": 4000,\n",
      "      \"prompt_token_pricing_per_million\": 1.5,\n",
      "      \"completion_token_pricing_per_million\": 2.0,\n",
      "      \"inference_param_set\": \"cohere\"\n",
      "    },\n",
      "    \"amazon.titan-text-express-v1\": {\n",
      "      \"context_length\": 8000,\n",
      "      \"prompt_token_pricing_per_million\": 0.8,\n",
      "      \"completion_token_pricing_per_million\": 1.6,\n",
      "      \"inference_param_set\": \"titan\"\n",
      "    },\n",
      "    \"anthropic.claude-v2:1\": {\n",
      "      \"context_length\": 100000,\n",
      "      \"prompt_token_pricing_per_million\": 8,\n",
      "      \"completion_token_pricing_per_million\": 24,\n",
      "      \"inference_param_set\": \"claude\"\n",
      "    },\n",
      "    \"anthropic.claude-instant-v1\": {\n",
      "      \"context_length\": 9000,\n",
      "      \"prompt_token_pricing_per_million\": 1.63,\n",
      "      \"completion_token_pricing_per_million\": 5.51,\n",
      "      \"inference_param_set\": \"claude\"\n",
      "    }\n",
      "  },\n",
      "  \"inference_params\": {\n",
      "    \"claude\": {\n",
      "      \"max_tokens_to_sample\": 512,\n",
      "      \"temperature\": 0.1,\n",
      "      \"top_k\": 250,\n",
      "      \"top_p\": 0.5,\n",
      "      \"stop_sequences\": [],\n",
      "      \"anthropic_version\": \"bedrock-2023-05-31\"\n",
      "    },\n",
      "    \"titan\": {\n",
      "      \"maxTokenCount\": 512,\n",
      "      \"stopSequences\": [],\n",
      "      \"temperature\": 0.1,\n",
      "      \"topP\": 0.9\n",
      "    },\n",
      "    \"cohere\": {\n",
      "      \"max_tokens\": 512,\n",
      "      \"temperature\": 0.1,\n",
      "      \"p\": 0.9,\n",
      "      \"k\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-12 19:35:57,925] p47653 {credentials.py:1278} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "## Initialize the bedrock client\n",
    "bedrock_client = get_bedrock_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-12 19:35:58,032] p47653 {credentials.py:1278} INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelArn</th>\n",
       "      <th>modelId</th>\n",
       "      <th>modelName</th>\n",
       "      <th>providerName</th>\n",
       "      <th>inputModalities</th>\n",
       "      <th>outputModalities</th>\n",
       "      <th>responseStreamingSupported</th>\n",
       "      <th>customizationsSupported</th>\n",
       "      <th>inferenceTypesSupported</th>\n",
       "      <th>modelLifecycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-tg1-large</td>\n",
       "      <td>Titan Text Large</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-e1t-medium</td>\n",
       "      <td>Titan Text Embeddings</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'LEGACY'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-image-generator-v1:0</td>\n",
       "      <td>Titan Image Generator G1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[IMAGE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[FINE_TUNING]</td>\n",
       "      <td>[ON_DEMAND, PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-image-generator-v1</td>\n",
       "      <td>Titan Image Generator G1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[IMAGE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-embed-g1-text-02</td>\n",
       "      <td>Titan Text Embeddings v2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-text-lite-v1:0:4k</td>\n",
       "      <td>Titan Text G1 - Lite</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[FINE_TUNING, CONTINUED_PRE_TRAINING]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-text-lite-v1</td>\n",
       "      <td>Titan Text G1 - Lite</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-text-express-v1:0:8k</td>\n",
       "      <td>Titan Text G1 - Express</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[FINE_TUNING, CONTINUED_PRE_TRAINING]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>Titan Text G1 - Express</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-embed-text-v1:2:8k</td>\n",
       "      <td>Titan Embeddings G1 - Text</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-embed-text-v1</td>\n",
       "      <td>Titan Embeddings G1 - Text</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-embed-image-v1:0</td>\n",
       "      <td>Titan Multimodal Embeddings G1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[FINE_TUNING]</td>\n",
       "      <td>[ON_DEMAND, PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/am...</td>\n",
       "      <td>amazon.titan-embed-image-v1</td>\n",
       "      <td>Titan Multimodal Embeddings G1</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/st...</td>\n",
       "      <td>stability.stable-diffusion-xl</td>\n",
       "      <td>SDXL 0.8</td>\n",
       "      <td>Stability AI</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[IMAGE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/st...</td>\n",
       "      <td>stability.stable-diffusion-xl-v0</td>\n",
       "      <td>SDXL 0.8</td>\n",
       "      <td>Stability AI</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[IMAGE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/st...</td>\n",
       "      <td>stability.stable-diffusion-xl-v1:0</td>\n",
       "      <td>SDXL 1.0</td>\n",
       "      <td>Stability AI</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[IMAGE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/st...</td>\n",
       "      <td>stability.stable-diffusion-xl-v1</td>\n",
       "      <td>SDXL 1.0</td>\n",
       "      <td>Stability AI</td>\n",
       "      <td>[TEXT, IMAGE]</td>\n",
       "      <td>[IMAGE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/ai...</td>\n",
       "      <td>ai21.j2-grande-instruct</td>\n",
       "      <td>J2 Grande Instruct</td>\n",
       "      <td>AI21 Labs</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/ai...</td>\n",
       "      <td>ai21.j2-jumbo-instruct</td>\n",
       "      <td>J2 Jumbo Instruct</td>\n",
       "      <td>AI21 Labs</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/ai...</td>\n",
       "      <td>ai21.j2-mid</td>\n",
       "      <td>Jurassic-2 Mid</td>\n",
       "      <td>AI21 Labs</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/ai...</td>\n",
       "      <td>ai21.j2-mid-v1</td>\n",
       "      <td>Jurassic-2 Mid</td>\n",
       "      <td>AI21 Labs</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/ai...</td>\n",
       "      <td>ai21.j2-ultra</td>\n",
       "      <td>Jurassic-2 Ultra</td>\n",
       "      <td>AI21 Labs</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/ai...</td>\n",
       "      <td>ai21.j2-ultra-v1</td>\n",
       "      <td>Jurassic-2 Ultra</td>\n",
       "      <td>AI21 Labs</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-instant-v1:2:100k</td>\n",
       "      <td>Claude Instant</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>Claude Instant</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-v1</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'LEGACY'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-v2:0:18k</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-v2:0:100k</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-v2:1:18k</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-v2:1:200k</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-v2:1</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/an...</td>\n",
       "      <td>anthropic.claude-v2</td>\n",
       "      <td>Claude</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/co...</td>\n",
       "      <td>cohere.command-text-v14:7:4k</td>\n",
       "      <td>Command</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[FINE_TUNING]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/co...</td>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>Command</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/co...</td>\n",
       "      <td>cohere.command-light-text-v14:7:4k</td>\n",
       "      <td>Command Light</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[FINE_TUNING]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/co...</td>\n",
       "      <td>cohere.command-light-text-v14</td>\n",
       "      <td>Command Light</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/co...</td>\n",
       "      <td>cohere.embed-english-v3</td>\n",
       "      <td>Embed English</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/co...</td>\n",
       "      <td>cohere.embed-multilingual-v3</td>\n",
       "      <td>Embed Multilingual</td>\n",
       "      <td>Cohere</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[EMBEDDING]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama2-13b-chat-v1:0:4k</td>\n",
       "      <td>Llama 2 Chat 13B</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PROVISIONED]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama2-13b-chat-v1</td>\n",
       "      <td>Llama 2 Chat 13B</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama2-70b-chat-v1:0:4k</td>\n",
       "      <td>Llama 2 Chat 70B</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama2-70b-chat-v1</td>\n",
       "      <td>Llama 2 Chat 70B</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ON_DEMAND]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama2-13b-v1:0:4k</td>\n",
       "      <td>Llama 2 13B</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[FINE_TUNING]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama2-13b-v1</td>\n",
       "      <td>Llama 2 13B</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama2-70b-v1:0:4k</td>\n",
       "      <td>Llama 2 70B</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[FINE_TUNING]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>arn:aws:bedrock:us-east-1::foundation-model/me...</td>\n",
       "      <td>meta.llama2-70b-v1</td>\n",
       "      <td>Llama 2 70B</td>\n",
       "      <td>Meta</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>[TEXT]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'status': 'ACTIVE'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             modelArn  \\\n",
       "0   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "1   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "2   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "3   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "4   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "5   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "6   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "7   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "8   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "9   arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "10  arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "11  arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "12  arn:aws:bedrock:us-east-1::foundation-model/am...   \n",
       "13  arn:aws:bedrock:us-east-1::foundation-model/st...   \n",
       "14  arn:aws:bedrock:us-east-1::foundation-model/st...   \n",
       "15  arn:aws:bedrock:us-east-1::foundation-model/st...   \n",
       "16  arn:aws:bedrock:us-east-1::foundation-model/st...   \n",
       "17  arn:aws:bedrock:us-east-1::foundation-model/ai...   \n",
       "18  arn:aws:bedrock:us-east-1::foundation-model/ai...   \n",
       "19  arn:aws:bedrock:us-east-1::foundation-model/ai...   \n",
       "20  arn:aws:bedrock:us-east-1::foundation-model/ai...   \n",
       "21  arn:aws:bedrock:us-east-1::foundation-model/ai...   \n",
       "22  arn:aws:bedrock:us-east-1::foundation-model/ai...   \n",
       "23  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "24  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "25  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "26  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "27  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "28  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "29  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "30  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "31  arn:aws:bedrock:us-east-1::foundation-model/an...   \n",
       "32  arn:aws:bedrock:us-east-1::foundation-model/co...   \n",
       "33  arn:aws:bedrock:us-east-1::foundation-model/co...   \n",
       "34  arn:aws:bedrock:us-east-1::foundation-model/co...   \n",
       "35  arn:aws:bedrock:us-east-1::foundation-model/co...   \n",
       "36  arn:aws:bedrock:us-east-1::foundation-model/co...   \n",
       "37  arn:aws:bedrock:us-east-1::foundation-model/co...   \n",
       "38  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "39  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "40  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "41  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "42  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "43  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "44  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "45  arn:aws:bedrock:us-east-1::foundation-model/me...   \n",
       "\n",
       "                               modelId                       modelName  \\\n",
       "0               amazon.titan-tg1-large                Titan Text Large   \n",
       "1              amazon.titan-e1t-medium           Titan Text Embeddings   \n",
       "2    amazon.titan-image-generator-v1:0        Titan Image Generator G1   \n",
       "3      amazon.titan-image-generator-v1        Titan Image Generator G1   \n",
       "4        amazon.titan-embed-g1-text-02        Titan Text Embeddings v2   \n",
       "5       amazon.titan-text-lite-v1:0:4k            Titan Text G1 - Lite   \n",
       "6            amazon.titan-text-lite-v1            Titan Text G1 - Lite   \n",
       "7    amazon.titan-text-express-v1:0:8k         Titan Text G1 - Express   \n",
       "8         amazon.titan-text-express-v1         Titan Text G1 - Express   \n",
       "9      amazon.titan-embed-text-v1:2:8k      Titan Embeddings G1 - Text   \n",
       "10          amazon.titan-embed-text-v1      Titan Embeddings G1 - Text   \n",
       "11       amazon.titan-embed-image-v1:0  Titan Multimodal Embeddings G1   \n",
       "12         amazon.titan-embed-image-v1  Titan Multimodal Embeddings G1   \n",
       "13       stability.stable-diffusion-xl                        SDXL 0.8   \n",
       "14    stability.stable-diffusion-xl-v0                        SDXL 0.8   \n",
       "15  stability.stable-diffusion-xl-v1:0                        SDXL 1.0   \n",
       "16    stability.stable-diffusion-xl-v1                        SDXL 1.0   \n",
       "17             ai21.j2-grande-instruct              J2 Grande Instruct   \n",
       "18              ai21.j2-jumbo-instruct               J2 Jumbo Instruct   \n",
       "19                         ai21.j2-mid                  Jurassic-2 Mid   \n",
       "20                      ai21.j2-mid-v1                  Jurassic-2 Mid   \n",
       "21                       ai21.j2-ultra                Jurassic-2 Ultra   \n",
       "22                    ai21.j2-ultra-v1                Jurassic-2 Ultra   \n",
       "23  anthropic.claude-instant-v1:2:100k                  Claude Instant   \n",
       "24         anthropic.claude-instant-v1                  Claude Instant   \n",
       "25                 anthropic.claude-v1                          Claude   \n",
       "26           anthropic.claude-v2:0:18k                          Claude   \n",
       "27          anthropic.claude-v2:0:100k                          Claude   \n",
       "28           anthropic.claude-v2:1:18k                          Claude   \n",
       "29          anthropic.claude-v2:1:200k                          Claude   \n",
       "30               anthropic.claude-v2:1                          Claude   \n",
       "31                 anthropic.claude-v2                          Claude   \n",
       "32        cohere.command-text-v14:7:4k                         Command   \n",
       "33             cohere.command-text-v14                         Command   \n",
       "34  cohere.command-light-text-v14:7:4k                   Command Light   \n",
       "35       cohere.command-light-text-v14                   Command Light   \n",
       "36             cohere.embed-english-v3                   Embed English   \n",
       "37        cohere.embed-multilingual-v3              Embed Multilingual   \n",
       "38        meta.llama2-13b-chat-v1:0:4k                Llama 2 Chat 13B   \n",
       "39             meta.llama2-13b-chat-v1                Llama 2 Chat 13B   \n",
       "40        meta.llama2-70b-chat-v1:0:4k                Llama 2 Chat 70B   \n",
       "41             meta.llama2-70b-chat-v1                Llama 2 Chat 70B   \n",
       "42             meta.llama2-13b-v1:0:4k                     Llama 2 13B   \n",
       "43                  meta.llama2-13b-v1                     Llama 2 13B   \n",
       "44             meta.llama2-70b-v1:0:4k                     Llama 2 70B   \n",
       "45                  meta.llama2-70b-v1                     Llama 2 70B   \n",
       "\n",
       "    providerName inputModalities outputModalities responseStreamingSupported  \\\n",
       "0         Amazon          [TEXT]           [TEXT]                       True   \n",
       "1         Amazon          [TEXT]      [EMBEDDING]                        NaN   \n",
       "2         Amazon   [TEXT, IMAGE]          [IMAGE]                        NaN   \n",
       "3         Amazon   [TEXT, IMAGE]          [IMAGE]                        NaN   \n",
       "4         Amazon          [TEXT]      [EMBEDDING]                        NaN   \n",
       "5         Amazon          [TEXT]           [TEXT]                       True   \n",
       "6         Amazon          [TEXT]           [TEXT]                       True   \n",
       "7         Amazon          [TEXT]           [TEXT]                       True   \n",
       "8         Amazon          [TEXT]           [TEXT]                       True   \n",
       "9         Amazon          [TEXT]      [EMBEDDING]                      False   \n",
       "10        Amazon          [TEXT]      [EMBEDDING]                      False   \n",
       "11        Amazon   [TEXT, IMAGE]      [EMBEDDING]                        NaN   \n",
       "12        Amazon   [TEXT, IMAGE]      [EMBEDDING]                        NaN   \n",
       "13  Stability AI   [TEXT, IMAGE]          [IMAGE]                        NaN   \n",
       "14  Stability AI   [TEXT, IMAGE]          [IMAGE]                        NaN   \n",
       "15  Stability AI   [TEXT, IMAGE]          [IMAGE]                        NaN   \n",
       "16  Stability AI   [TEXT, IMAGE]          [IMAGE]                        NaN   \n",
       "17     AI21 Labs          [TEXT]           [TEXT]                      False   \n",
       "18     AI21 Labs          [TEXT]           [TEXT]                      False   \n",
       "19     AI21 Labs          [TEXT]           [TEXT]                      False   \n",
       "20     AI21 Labs          [TEXT]           [TEXT]                      False   \n",
       "21     AI21 Labs          [TEXT]           [TEXT]                      False   \n",
       "22     AI21 Labs          [TEXT]           [TEXT]                      False   \n",
       "23     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "24     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "25     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "26     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "27     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "28     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "29     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "30     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "31     Anthropic          [TEXT]           [TEXT]                       True   \n",
       "32        Cohere          [TEXT]           [TEXT]                       True   \n",
       "33        Cohere          [TEXT]           [TEXT]                       True   \n",
       "34        Cohere          [TEXT]           [TEXT]                       True   \n",
       "35        Cohere          [TEXT]           [TEXT]                       True   \n",
       "36        Cohere          [TEXT]      [EMBEDDING]                      False   \n",
       "37        Cohere          [TEXT]      [EMBEDDING]                      False   \n",
       "38          Meta          [TEXT]           [TEXT]                       True   \n",
       "39          Meta          [TEXT]           [TEXT]                       True   \n",
       "40          Meta          [TEXT]           [TEXT]                       True   \n",
       "41          Meta          [TEXT]           [TEXT]                       True   \n",
       "42          Meta          [TEXT]           [TEXT]                       True   \n",
       "43          Meta          [TEXT]           [TEXT]                       True   \n",
       "44          Meta          [TEXT]           [TEXT]                       True   \n",
       "45          Meta          [TEXT]           [TEXT]                       True   \n",
       "\n",
       "                  customizationsSupported   inferenceTypesSupported  \\\n",
       "0                                      []               [ON_DEMAND]   \n",
       "1                                      []               [ON_DEMAND]   \n",
       "2                           [FINE_TUNING]  [ON_DEMAND, PROVISIONED]   \n",
       "3                                      []               [ON_DEMAND]   \n",
       "4                                      []               [ON_DEMAND]   \n",
       "5   [FINE_TUNING, CONTINUED_PRE_TRAINING]             [PROVISIONED]   \n",
       "6                                      []               [ON_DEMAND]   \n",
       "7   [FINE_TUNING, CONTINUED_PRE_TRAINING]             [PROVISIONED]   \n",
       "8                                      []               [ON_DEMAND]   \n",
       "9                                      []             [PROVISIONED]   \n",
       "10                                     []               [ON_DEMAND]   \n",
       "11                          [FINE_TUNING]  [ON_DEMAND, PROVISIONED]   \n",
       "12                                     []               [ON_DEMAND]   \n",
       "13                                     []               [ON_DEMAND]   \n",
       "14                                     []               [ON_DEMAND]   \n",
       "15                                     []             [PROVISIONED]   \n",
       "16                                     []               [ON_DEMAND]   \n",
       "17                                     []               [ON_DEMAND]   \n",
       "18                                     []               [ON_DEMAND]   \n",
       "19                                     []               [ON_DEMAND]   \n",
       "20                                     []               [ON_DEMAND]   \n",
       "21                                     []               [ON_DEMAND]   \n",
       "22                                     []               [ON_DEMAND]   \n",
       "23                                     []             [PROVISIONED]   \n",
       "24                                     []               [ON_DEMAND]   \n",
       "25                                     []               [ON_DEMAND]   \n",
       "26                                     []             [PROVISIONED]   \n",
       "27                                     []             [PROVISIONED]   \n",
       "28                                     []             [PROVISIONED]   \n",
       "29                                     []             [PROVISIONED]   \n",
       "30                                     []               [ON_DEMAND]   \n",
       "31                                     []               [ON_DEMAND]   \n",
       "32                          [FINE_TUNING]             [PROVISIONED]   \n",
       "33                                     []               [ON_DEMAND]   \n",
       "34                          [FINE_TUNING]             [PROVISIONED]   \n",
       "35                                     []               [ON_DEMAND]   \n",
       "36                                     []               [ON_DEMAND]   \n",
       "37                                     []               [ON_DEMAND]   \n",
       "38                                     []             [PROVISIONED]   \n",
       "39                                     []               [ON_DEMAND]   \n",
       "40                                     []                        []   \n",
       "41                                     []               [ON_DEMAND]   \n",
       "42                          [FINE_TUNING]                        []   \n",
       "43                                     []                        []   \n",
       "44                          [FINE_TUNING]                        []   \n",
       "45                                     []                        []   \n",
       "\n",
       "          modelLifecycle  \n",
       "0   {'status': 'ACTIVE'}  \n",
       "1   {'status': 'LEGACY'}  \n",
       "2   {'status': 'ACTIVE'}  \n",
       "3   {'status': 'ACTIVE'}  \n",
       "4   {'status': 'ACTIVE'}  \n",
       "5   {'status': 'ACTIVE'}  \n",
       "6   {'status': 'ACTIVE'}  \n",
       "7   {'status': 'ACTIVE'}  \n",
       "8   {'status': 'ACTIVE'}  \n",
       "9   {'status': 'ACTIVE'}  \n",
       "10  {'status': 'ACTIVE'}  \n",
       "11  {'status': 'ACTIVE'}  \n",
       "12  {'status': 'ACTIVE'}  \n",
       "13  {'status': 'ACTIVE'}  \n",
       "14  {'status': 'ACTIVE'}  \n",
       "15  {'status': 'ACTIVE'}  \n",
       "16  {'status': 'ACTIVE'}  \n",
       "17  {'status': 'ACTIVE'}  \n",
       "18  {'status': 'ACTIVE'}  \n",
       "19  {'status': 'ACTIVE'}  \n",
       "20  {'status': 'ACTIVE'}  \n",
       "21  {'status': 'ACTIVE'}  \n",
       "22  {'status': 'ACTIVE'}  \n",
       "23  {'status': 'ACTIVE'}  \n",
       "24  {'status': 'ACTIVE'}  \n",
       "25  {'status': 'LEGACY'}  \n",
       "26  {'status': 'ACTIVE'}  \n",
       "27  {'status': 'ACTIVE'}  \n",
       "28  {'status': 'ACTIVE'}  \n",
       "29  {'status': 'ACTIVE'}  \n",
       "30  {'status': 'ACTIVE'}  \n",
       "31  {'status': 'ACTIVE'}  \n",
       "32  {'status': 'ACTIVE'}  \n",
       "33  {'status': 'ACTIVE'}  \n",
       "34  {'status': 'ACTIVE'}  \n",
       "35  {'status': 'ACTIVE'}  \n",
       "36  {'status': 'ACTIVE'}  \n",
       "37  {'status': 'ACTIVE'}  \n",
       "38  {'status': 'ACTIVE'}  \n",
       "39  {'status': 'ACTIVE'}  \n",
       "40  {'status': 'ACTIVE'}  \n",
       "41  {'status': 'ACTIVE'}  \n",
       "42  {'status': 'ACTIVE'}  \n",
       "43  {'status': 'ACTIVE'}  \n",
       "44  {'status': 'ACTIVE'}  \n",
       "45  {'status': 'ACTIVE'}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boto3_bedrock = get_bedrock_client(runtime=False)\n",
    "fm_list_response = boto3_bedrock.list_foundation_models()\n",
    "fm_list = fm_list_response['modelSummaries']\n",
    "df_fm = pd.DataFrame(fm_list)\n",
    "display(df_fm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-12 19:35:58,309] p47653 {3139140113.py:2} INFO - found 5 transcript_files ->\n",
      "['data/raw/0/call_center_transcript_1_transcript.txt', 'data/raw/1/call_center_transcript_0_transcript.txt', 'data/raw/4/call_center_transcript_4_transcript.txt', 'data/raw/3/call_center_transcript_3_transcript.txt', 'data/raw/2/call_center_transcript_2_transcript.txt']\n"
     ]
    }
   ],
   "source": [
    "transcript_files = glob.glob(os.path.join(config['dir']['raw'], \"*\", \"*transcript.txt\"))\n",
    "logger.info(f\"found {len(transcript_files)} transcript_files ->\\n{transcript_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/madhurpt/Downloads/bedrock-contact-center-tasks-eval-main-3/utils.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Re load the utils file to make sure all functions are included from the utils class before importing them\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model payloads per transcript per bedrock model ->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_id': 'amazon.titan-text-express-v1',\n",
       "  'payload': {'inputText': \"A: I wanted to discuss our strategy around generative AI and how we should approach this emerging technology. As you know, several applications have captured a lot of attention recently. \\nAction item: Set up a follow-up meeting to brainstorm ideas for where generative AI could be applicable in our products\\n\\nB: Yes, generative AI is definitely a hot topic right now. All the major tech companies seem to be investing heavily in this space.\\nAction item: Research current generative AI initiatives at other tech companies to analyze the competitive landscape\\n\\nA: Exactly. I think we need to have a plan here too or risk falling behind. What kind of applications do you see for generative AI in our products? Could it be used to automate certain processes or enhance our users' experience?\\nAction item: Outline high-level ideas for where generative AI could drive automation or enhance user experience in our products\\n\\nC: There's certainly potential, but we'd need to proceed cautiously. While the capabilities are impressive, the technology still has problems with accuracy and bias that could negatively impact customers if we don't validate it thoroughly before rolling anything out.\\nAction item: Develop a validation framework to rigorously assess accuracy, ethics and safety of any generative AI applications we develop\\n\\nB: I agree. We'll want to set clear guidelines for what generative AI should and\"}},\n",
       " {'model_id': 'anthropic.claude-instant-v1',\n",
       "  'payload': {'prompt': \"\\n\\nHuman: A: I wanted to discuss our strategy around generative AI and how we should approach this emerging technology. As you know, several applications have captured a lot of attention recently. \\nAction item: Set up a follow-up meeting to brainstorm ideas for where generative AI could be applicable in our products\\n\\nB: Yes, generative AI is definitely a hot topic right now. All the major tech companies seem to be investing heavily in this space.\\nAction item: Research current generative AI initiatives at other tech companies to analyze the competitive landscape\\n\\nA: Exactly. I think we need to have a plan here too or risk falling behind. What kind of applications do you see for generative AI in our products? Could it be used to automate certain processes or enhance our users' experience?\\nAction item: Outline high-level ideas for where generative AI could drive automation or enhance user experience in our products\\n\\nC: There's certainly potential, but we'd need to proceed cautiously. While the capabilities are impressive, the technology still has problems with accuracy and bias that could negatively impact customers if we don't validate it thoroughly before rolling anything out.\\nAction item: Develop a validation framework to rigorously assess accuracy, ethics and safety of any generative AI applications we develop\\n\\nB: I agree. We'll want to set clear guidelines for what generative AI should and\\n\\nAssistant:\",\n",
       "   'max_tokens_to_sample': 300,\n",
       "   'temperature': 0.5,\n",
       "   'top_k': 250,\n",
       "   'top_p': 1}},\n",
       " {'model_id': 'cohere.command-text-v14',\n",
       "  'payload': {'prompt': \"A: I wanted to discuss our strategy around generative AI and how we should approach this emerging technology. As you know, several applications have captured a lot of attention recently. \\nAction item: Set up a follow-up meeting to brainstorm ideas for where generative AI could be applicable in our products\\n\\nB: Yes, generative AI is definitely a hot topic right now. All the major tech companies seem to be investing heavily in this space.\\nAction item: Research current generative AI initiatives at other tech companies to analyze the competitive landscape\\n\\nA: Exactly. I think we need to have a plan here too or risk falling behind. What kind of applications do you see for generative AI in our products? Could it be used to automate certain processes or enhance our users' experience?\\nAction item: Outline high-level ideas for where generative AI could drive automation or enhance user experience in our products\\n\\nC: There's certainly potential, but we'd need to proceed cautiously. While the capabilities are impressive, the technology still has problems with accuracy and bias that could negatively impact customers if we don't validate it thoroughly before rolling anything out.\\nAction item: Develop a validation framework to rigorously assess accuracy, ethics and safety of any generative AI applications we develop\\n\\nB: I agree. We'll want to set clear guidelines for what generative AI should and\",\n",
       "   'max_tokens': 100,\n",
       "   'temperature': 0.8}},\n",
       " {'model_id': 'amazon.titan-text-express-v1',\n",
       "  'payload': {'inputText': 'Meeting transcript: \\nA: Hi B, I want to discuss the workstream for our new product launch\\nB: Sure A, is there anything in particular you want to discuss?\\nA: Yes, I want to talk about how users enter into the product.  \\nB: Ok, in that case let me add in C.\\nC: Hey everyone\\nB: Hi C, A wants to discuss how users enter into the product.\\nA: its too complicated and we should remove friction.  for example, why do I need to fill out additional forms? I also find it difficult to find where to access the product when I first land on the landing page.  \\nB: I would also add that I think there are too many steps.\\nC: Ok, I can work on the landing page to make the product more discoverable but B can you work on the additonal forms?\\nB: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow. A can you document any other concerns so that I can discuss with James only once?\\nA: Sure.'}},\n",
       " {'model_id': 'anthropic.claude-instant-v1',\n",
       "  'payload': {'prompt': '\\n\\nHuman: Meeting transcript: \\nA: Hi B, I want to discuss the workstream for our new product launch\\nB: Sure A, is there anything in particular you want to discuss?\\nA: Yes, I want to talk about how users enter into the product.  \\nB: Ok, in that case let me add in C.\\nC: Hey everyone\\nB: Hi C, A wants to discuss how users enter into the product.\\nA: its too complicated and we should remove friction.  for example, why do I need to fill out additional forms? I also find it difficult to find where to access the product when I first land on the landing page.  \\nB: I would also add that I think there are too many steps.\\nC: Ok, I can work on the landing page to make the product more discoverable but B can you work on the additonal forms?\\nB: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow. A can you document any other concerns so that I can discuss with James only once?\\nA: Sure.\\n\\nAssistant:',\n",
       "   'max_tokens_to_sample': 300,\n",
       "   'temperature': 0.5,\n",
       "   'top_k': 250,\n",
       "   'top_p': 1}},\n",
       " {'model_id': 'cohere.command-text-v14',\n",
       "  'payload': {'prompt': 'Meeting transcript: \\nA: Hi B, I want to discuss the workstream for our new product launch\\nB: Sure A, is there anything in particular you want to discuss?\\nA: Yes, I want to talk about how users enter into the product.  \\nB: Ok, in that case let me add in C.\\nC: Hey everyone\\nB: Hi C, A wants to discuss how users enter into the product.\\nA: its too complicated and we should remove friction.  for example, why do I need to fill out additional forms? I also find it difficult to find where to access the product when I first land on the landing page.  \\nB: I would also add that I think there are too many steps.\\nC: Ok, I can work on the landing page to make the product more discoverable but B can you work on the additonal forms?\\nB: Yes but I would need to work with James from another team as he needs to unblock the sign up workflow. A can you document any other concerns so that I can discuss with James only once?\\nA: Sure.',\n",
       "   'max_tokens': 100,\n",
       "   'temperature': 0.8}},\n",
       " {'model_id': 'amazon.titan-text-express-v1',\n",
       "  'payload': {'inputText': \"A: I wanted to further discuss options for the optimal cloud instance type to host our new product recommendation model. As a refresher on the requirements - this model will be query intensive, with thousands of customers hitting the prediction API simultaneously. And it relies on a large deep neural network for the recommendations. \\n\\nB: Yes, we should go through the pros and cons again of the main instance family options available. For standard instance types without GPUs, we'd likely need to go with a high CPU and memory specification to handle the computation and concurrency needs.\\n\\nC: Standard instances would be more cost-effective, but prediction latency would suffer without GPU acceleration. The neural network calculations could be 5-10x slower based on our initial benchmarks. For a customer-facing model, slow predictions could lead to poor user experience.\\n\\nA: That's a very good point. While GPU-powered instances would be more expensive, the business value of sub-second prediction latency for users is high. We want customers to get engaging, personalized recommendations immediately to drive conversions.\\n\\nB: I agree, I think GPU acceleration makes more sense despite the increased cloud costs. In addition to the deep learning performance boost, some of the other model parallelization and tuning optimizations we're doing would also run faster on GPUs.\\n\\nC: It seems like GPU instances are the way to go then as long as the total monthly cost is affordable. However, we may be able to optimize the specific type of GPU instance to find the best balance of throughput versus cost. \\n\\nA: Definitely, the major cloud providers offer an array of GPU instance types optimized for different use cases. We should benchmark performance against our workload with NVIDIA T4 GPUs, which are cost-optimized options, as well as some higher-end options like A100 GPUs.\\n\\nC: Do you think we should also explore using something like Elastic Inference GPU attachments? Those let you augment existing instances with fractional GPU access that is more cost-effective.\\n\\nB: Hmm good question. In theory that could work well for the cost-performance balance. We'd need to test compatibility with the types of GPU optimizations we're running in the model though...\"}},\n",
       " {'model_id': 'anthropic.claude-instant-v1',\n",
       "  'payload': {'prompt': \"\\n\\nHuman: A: I wanted to further discuss options for the optimal cloud instance type to host our new product recommendation model. As a refresher on the requirements - this model will be query intensive, with thousands of customers hitting the prediction API simultaneously. And it relies on a large deep neural network for the recommendations. \\n\\nB: Yes, we should go through the pros and cons again of the main instance family options available. For standard instance types without GPUs, we'd likely need to go with a high CPU and memory specification to handle the computation and concurrency needs.\\n\\nC: Standard instances would be more cost-effective, but prediction latency would suffer without GPU acceleration. The neural network calculations could be 5-10x slower based on our initial benchmarks. For a customer-facing model, slow predictions could lead to poor user experience.\\n\\nA: That's a very good point. While GPU-powered instances would be more expensive, the business value of sub-second prediction latency for users is high. We want customers to get engaging, personalized recommendations immediately to drive conversions.\\n\\nB: I agree, I think GPU acceleration makes more sense despite the increased cloud costs. In addition to the deep learning performance boost, some of the other model parallelization and tuning optimizations we're doing would also run faster on GPUs.\\n\\nC: It seems like GPU instances are the way to go then as long as the total monthly cost is affordable. However, we may be able to optimize the specific type of GPU instance to find the best balance of throughput versus cost. \\n\\nA: Definitely, the major cloud providers offer an array of GPU instance types optimized for different use cases. We should benchmark performance against our workload with NVIDIA T4 GPUs, which are cost-optimized options, as well as some higher-end options like A100 GPUs.\\n\\nC: Do you think we should also explore using something like Elastic Inference GPU attachments? Those let you augment existing instances with fractional GPU access that is more cost-effective.\\n\\nB: Hmm good question. In theory that could work well for the cost-performance balance. We'd need to test compatibility with the types of GPU optimizations we're running in the model though...\\n\\nAssistant:\",\n",
       "   'max_tokens_to_sample': 300,\n",
       "   'temperature': 0.5,\n",
       "   'top_k': 250,\n",
       "   'top_p': 1}},\n",
       " {'model_id': 'cohere.command-text-v14',\n",
       "  'payload': {'prompt': \"A: I wanted to further discuss options for the optimal cloud instance type to host our new product recommendation model. As a refresher on the requirements - this model will be query intensive, with thousands of customers hitting the prediction API simultaneously. And it relies on a large deep neural network for the recommendations. \\n\\nB: Yes, we should go through the pros and cons again of the main instance family options available. For standard instance types without GPUs, we'd likely need to go with a high CPU and memory specification to handle the computation and concurrency needs.\\n\\nC: Standard instances would be more cost-effective, but prediction latency would suffer without GPU acceleration. The neural network calculations could be 5-10x slower based on our initial benchmarks. For a customer-facing model, slow predictions could lead to poor user experience.\\n\\nA: That's a very good point. While GPU-powered instances would be more expensive, the business value of sub-second prediction latency for users is high. We want customers to get engaging, personalized recommendations immediately to drive conversions.\\n\\nB: I agree, I think GPU acceleration makes more sense despite the increased cloud costs. In addition to the deep learning performance boost, some of the other model parallelization and tuning optimizations we're doing would also run faster on GPUs.\\n\\nC: It seems like GPU instances are the way to go then as long as the total monthly cost is affordable. However, we may be able to optimize the specific type of GPU instance to find the best balance of throughput versus cost. \\n\\nA: Definitely, the major cloud providers offer an array of GPU instance types optimized for different use cases. We should benchmark performance against our workload with NVIDIA T4 GPUs, which are cost-optimized options, as well as some higher-end options like A100 GPUs.\\n\\nC: Do you think we should also explore using something like Elastic Inference GPU attachments? Those let you augment existing instances with fractional GPU access that is more cost-effective.\\n\\nB: Hmm good question. In theory that could work well for the cost-performance balance. We'd need to test compatibility with the types of GPU optimizations we're running in the model though...\",\n",
       "   'max_tokens': 100,\n",
       "   'temperature': 0.8}},\n",
       " {'model_id': 'amazon.titan-text-express-v1',\n",
       "  'payload': {'inputText': \"A: I wanted to have a broader discussion on responsible AI development principles. As we build more AI models, how do we ensure they are helpful, harmless, and aligned with human values?\\n\\nB: This is an important topic. We need to make ethical considerations a priority from the beginning of any AI project, not an afterthought. I think we should develop a review process focused on safety for all new models. \\n\\nAction item: Draft a proposal for an AI model safety review framework \\n\\nC: I agree. We should analyze potential risks like data bias and misuse upfront. Models also need to be secure - what if they were hacked?\\n\\nAction item: Outline additional potential safety risks to address such as data/algorithm bias and security\\n\\nA: Valid points. Besides analyzing risks, we also need to ensure the objectives we give AI align with human values more broadly. Like being helpful to users, not just maximizing some narrow metric. \\n\\nAction item: Explore methodologies for value alignment in AI systems\\n\\nB: The objectives point connects to transparency too. Any AI decisions that impact users should be explainable. If we can't understand why an AI did something, we can't verify it's aligned.  \\n\\nAction item: Research existing XAI (explainable AI) techniques to augment our models\\n\\nC: Adding explainability makes sense. We should also discuss how to carefully test models before launch to catch additional issues. And have a plan to monitor their impacts over time.\\n\\nAction item: Develop testing and monitoring standards for AI models based on industry best practices\\n\\nA: Great suggestions all around. It seems like we have a blueprint to make responsibility, safety and ethics central to our AI efforts here. I'm optimistic if we build these considerations in from the start, we can drive real progress. Does anyone have any other thoughts before we break?\"}},\n",
       " {'model_id': 'anthropic.claude-instant-v1',\n",
       "  'payload': {'prompt': \"\\n\\nHuman: A: I wanted to have a broader discussion on responsible AI development principles. As we build more AI models, how do we ensure they are helpful, harmless, and aligned with human values?\\n\\nB: This is an important topic. We need to make ethical considerations a priority from the beginning of any AI project, not an afterthought. I think we should develop a review process focused on safety for all new models. \\n\\nAction item: Draft a proposal for an AI model safety review framework \\n\\nC: I agree. We should analyze potential risks like data bias and misuse upfront. Models also need to be secure - what if they were hacked?\\n\\nAction item: Outline additional potential safety risks to address such as data/algorithm bias and security\\n\\nA: Valid points. Besides analyzing risks, we also need to ensure the objectives we give AI align with human values more broadly. Like being helpful to users, not just maximizing some narrow metric. \\n\\nAction item: Explore methodologies for value alignment in AI systems\\n\\nB: The objectives point connects to transparency too. Any AI decisions that impact users should be explainable. If we can't understand why an AI did something, we can't verify it's aligned.  \\n\\nAction item: Research existing XAI (explainable AI) techniques to augment our models\\n\\nC: Adding explainability makes sense. We should also discuss how to carefully test models before launch to catch additional issues. And have a plan to monitor their impacts over time.\\n\\nAction item: Develop testing and monitoring standards for AI models based on industry best practices\\n\\nA: Great suggestions all around. It seems like we have a blueprint to make responsibility, safety and ethics central to our AI efforts here. I'm optimistic if we build these considerations in from the start, we can drive real progress. Does anyone have any other thoughts before we break?\\n\\nAssistant:\",\n",
       "   'max_tokens_to_sample': 300,\n",
       "   'temperature': 0.5,\n",
       "   'top_k': 250,\n",
       "   'top_p': 1}},\n",
       " {'model_id': 'cohere.command-text-v14',\n",
       "  'payload': {'prompt': \"A: I wanted to have a broader discussion on responsible AI development principles. As we build more AI models, how do we ensure they are helpful, harmless, and aligned with human values?\\n\\nB: This is an important topic. We need to make ethical considerations a priority from the beginning of any AI project, not an afterthought. I think we should develop a review process focused on safety for all new models. \\n\\nAction item: Draft a proposal for an AI model safety review framework \\n\\nC: I agree. We should analyze potential risks like data bias and misuse upfront. Models also need to be secure - what if they were hacked?\\n\\nAction item: Outline additional potential safety risks to address such as data/algorithm bias and security\\n\\nA: Valid points. Besides analyzing risks, we also need to ensure the objectives we give AI align with human values more broadly. Like being helpful to users, not just maximizing some narrow metric. \\n\\nAction item: Explore methodologies for value alignment in AI systems\\n\\nB: The objectives point connects to transparency too. Any AI decisions that impact users should be explainable. If we can't understand why an AI did something, we can't verify it's aligned.  \\n\\nAction item: Research existing XAI (explainable AI) techniques to augment our models\\n\\nC: Adding explainability makes sense. We should also discuss how to carefully test models before launch to catch additional issues. And have a plan to monitor their impacts over time.\\n\\nAction item: Develop testing and monitoring standards for AI models based on industry best practices\\n\\nA: Great suggestions all around. It seems like we have a blueprint to make responsibility, safety and ethics central to our AI efforts here. I'm optimistic if we build these considerations in from the start, we can drive real progress. Does anyone have any other thoughts before we break?\",\n",
       "   'max_tokens': 100,\n",
       "   'temperature': 0.8}},\n",
       " {'model_id': 'amazon.titan-text-express-v1',\n",
       "  'payload': {'inputText': 'A: I wanted to discuss ideas for a new startup in the mobile gaming space. The success of companies like Angry Birds shows there is a lot of potential still untapped. \\nAction item: Research recent trends and growth in mobile gaming to identify opportunities\\n\\nB: I agree mobile gaming is hot right now. Have you thought about potential types of games we could develop? Should we try freemium or paid apps?\\nAction item: Outline pros and cons for freemium vs paid monetization models\\n\\nC: We should consider other business models too like in-game ads or sponsorships. The key is retaining users and driving high engagement.\\nAction item: Explore in-game ad networks and pricing\\nAction item: Brainstorm ideas to maximize user retention \\n\\nA: Those are good points. I also think we need to prototype game concepts quickly to validate demand before investing too heavily in development.\\nAction item: Define process for rapidly prototyping and testing game concepts  \\n\\nB: Yes, we could even use some AI tools now to auto-generate simple game artwork and mechanics. This can help speed up testing. \\nAction item: Evaluate AI-powered game prototyping tools to complement our development efforts\\n\\nC: Automating pieces through AI makes sense. Though we still need to ensure quality bar for any games we officially launch.\\n'}},\n",
       " {'model_id': 'anthropic.claude-instant-v1',\n",
       "  'payload': {'prompt': '\\n\\nHuman: A: I wanted to discuss ideas for a new startup in the mobile gaming space. The success of companies like Angry Birds shows there is a lot of potential still untapped. \\nAction item: Research recent trends and growth in mobile gaming to identify opportunities\\n\\nB: I agree mobile gaming is hot right now. Have you thought about potential types of games we could develop? Should we try freemium or paid apps?\\nAction item: Outline pros and cons for freemium vs paid monetization models\\n\\nC: We should consider other business models too like in-game ads or sponsorships. The key is retaining users and driving high engagement.\\nAction item: Explore in-game ad networks and pricing\\nAction item: Brainstorm ideas to maximize user retention \\n\\nA: Those are good points. I also think we need to prototype game concepts quickly to validate demand before investing too heavily in development.\\nAction item: Define process for rapidly prototyping and testing game concepts  \\n\\nB: Yes, we could even use some AI tools now to auto-generate simple game artwork and mechanics. This can help speed up testing. \\nAction item: Evaluate AI-powered game prototyping tools to complement our development efforts\\n\\nC: Automating pieces through AI makes sense. Though we still need to ensure quality bar for any games we officially launch.\\n\\n\\nAssistant:',\n",
       "   'max_tokens_to_sample': 300,\n",
       "   'temperature': 0.5,\n",
       "   'top_k': 250,\n",
       "   'top_p': 1}},\n",
       " {'model_id': 'cohere.command-text-v14',\n",
       "  'payload': {'prompt': 'A: I wanted to discuss ideas for a new startup in the mobile gaming space. The success of companies like Angry Birds shows there is a lot of potential still untapped. \\nAction item: Research recent trends and growth in mobile gaming to identify opportunities\\n\\nB: I agree mobile gaming is hot right now. Have you thought about potential types of games we could develop? Should we try freemium or paid apps?\\nAction item: Outline pros and cons for freemium vs paid monetization models\\n\\nC: We should consider other business models too like in-game ads or sponsorships. The key is retaining users and driving high engagement.\\nAction item: Explore in-game ad networks and pricing\\nAction item: Brainstorm ideas to maximize user retention \\n\\nA: Those are good points. I also think we need to prototype game concepts quickly to validate demand before investing too heavily in development.\\nAction item: Define process for rapidly prototyping and testing game concepts  \\n\\nB: Yes, we could even use some AI tools now to auto-generate simple game artwork and mechanics. This can help speed up testing. \\nAction item: Evaluate AI-powered game prototyping tools to complement our development efforts\\n\\nC: Automating pieces through AI makes sense. Though we still need to ensure quality bar for any games we officially launch.\\n',\n",
       "   'max_tokens': 100,\n",
       "   'temperature': 0.8}}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from json import JSONEncoder\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "import json\n",
    "from create_payload import model_payloads\n",
    "\n",
    "\n",
    "\n",
    "## Create a payload referring to the model_payloads script\n",
    "def create_payloads_for_all_models(transcript_files, config):\n",
    "\n",
    "    ## initialize a payload dict\n",
    "    all_payloads = []\n",
    "\n",
    "    ## Iterate through all of the transcript files available\n",
    "    for idx, tf in enumerate(transcript_files):\n",
    "        transcript = Path(tf).read_text()\n",
    "        fname = os.path.basename(tf)\n",
    "        file_id = \"_\".join(fname.split('_')[:-1])\n",
    "\n",
    "        ## Iterate through every experiment for each transcript file\n",
    "        for experiment in config['experiments']:\n",
    "            exp_name = experiment['name']\n",
    "            model_list = experiment['model_list']\n",
    "            \n",
    "            ## Iterate through each model for each transcript file\n",
    "            for model_info in model_list:\n",
    "                model_name = model_info['model']\n",
    "                model = config['bedrock_models'].get(model_name)\n",
    "\n",
    "                if model is None:\n",
    "                    logger.error(f\"model={model_name} not found in bedrock_models\")\n",
    "                    continue\n",
    "                \n",
    "                # Use the imported create_payload function to generate the payload\n",
    "                payload_dict = {\n",
    "                    \"model_id\": model_name,\n",
    "                    \"payload\": model_payloads(transcript, model_name)\n",
    "                }\n",
    "                all_payloads.append(payload_dict)\n",
    "\n",
    "    return all_payloads\n",
    "\n",
    "print(\"All model payloads per transcript per bedrock model ->\")\n",
    "create_payloads_for_all_models(transcript_files, config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import requests as req\n",
    "import botocore.session\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "from typing import Dict, List\n",
    "\n",
    "SERVICE_NAME = 'bedrock'\n",
    "region = 'us-east-1'\n",
    "\n",
    "## Utilizing the bedrock REST API to get inference from each bedrock model\n",
    "def get_inference(model_id: str, payload: Dict) -> Dict:\n",
    "    try:\n",
    "        endpoint = f\"https://{SERVICE_NAME}-runtime.{region}.amazonaws.com/model/{model_id}/invoke\"\n",
    "        request_body = json.dumps(payload)\n",
    "\n",
    "        request = AWSRequest(method='POST', url=endpoint, data=request_body, headers={'content-type': 'application/json'})\n",
    "        session = botocore.session.Session()\n",
    "        sigv4 = SigV4Auth(session.get_credentials(), SERVICE_NAME, region)\n",
    "        sigv4.add_auth(request)\n",
    "        prepped = request.prepare()\n",
    "\n",
    "        response = req.post(prepped.url, headers=prepped.headers, data=request_body)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error: Received status code {response.status_code}, Response: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "## Utilize the async calls for creating a thread of model invocations\n",
    "async def async_calls_on_model(model_id, payload):\n",
    "    try:\n",
    "        response = await asyncio.to_thread(get_inference, model_id, payload)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in async_calls_on_model: {e}\")\n",
    "        return None\n",
    "\n",
    "async def async_invoke_model(model_name, payloads):\n",
    "    responses = []\n",
    "    for payload in payloads:\n",
    "        response = await async_calls_on_model(model_name, payload)\n",
    "        responses.append(response)\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tokenizer_utils\n",
    "from tokenizer_utils import count_tokens\n",
    "\n",
    "## Filter the model payloads by the type of model id offering at bedrock for simpler calling\n",
    "def filter_payloads_for_model(model_name, all_payloads):\n",
    "    return [payload for payload in all_payloads if payload['model_id'] == model_name]\n",
    "\n",
    "## Function to process a model through invoking it with transcripts through various concurrencies in config.yml\n",
    "async def process_model(model_info, all_payloads, csv_writer, concurrency_level):\n",
    "    model_id = model_info['model']\n",
    "    ## filter the model by the id\n",
    "    model_payloads = filter_payloads_for_model(model_id, all_payloads)\n",
    "\n",
    "    ## loop through the conc levels and the model_payloads\n",
    "    for i in range(0, len(model_payloads), concurrency_level):\n",
    "        batch_payloads = [payload['payload'] for payload in model_payloads[i:i + concurrency_level]]\n",
    "        print(f\"Running {model_id} at concurrency level {concurrency_level}...\")\n",
    "\n",
    "        ## track metrics: latency\n",
    "        start_time = time.time()\n",
    "        responses = await async_invoke_model(model_id, batch_payloads)\n",
    "        end_time = time.time()\n",
    "        latency = (end_time - start_time)  # in seconds\n",
    "\n",
    "        # Log and write each response to the CSV\n",
    "        for j, response in enumerate(responses):\n",
    "            csv_writer.writerow({\n",
    "                'Model_id': model_id,\n",
    "                'input token count': count_tokens(f'{batch_payloads[j]}'),\n",
    "                'Latency (seconds)': latency,\n",
    "                'Concurrency Level': concurrency_level,\n",
    "                'call transcript': batch_payloads[j],\n",
    "                'Response': json.dumps(response)\n",
    "            })\n",
    "            \n",
    "            print(f\"Response {j+1} at concurrency level {concurrency_level}: {response}\")\n",
    "\n",
    "        print(f\"Latency: {latency} seconds\\n\")\n",
    "\n",
    "## save to csv function\n",
    "def init_csv_file(csv_file_path):\n",
    "    with open(csv_file_path, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['Model_id', 'input token count', 'Latency (seconds)', 'Concurrency Level', 'call transcript', 'Response'])\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "## Function to process all of the models to invoke all transcripts one by one based on the concurrency level\n",
    "async def process_all_models(all_payloads, config, csv_file_path):\n",
    "    with open(csv_file_path, 'a', newline='') as file:\n",
    "        csv_writer = csv.DictWriter(file, fieldnames=['Model_id', 'input token count', 'Latency (seconds)', 'Concurrency Level', 'call transcript', 'Response'])\n",
    "\n",
    "        ## Loop through each concurrency level for each experiment and each model, and create tasks to call process model on the model id and payloads\n",
    "        for concurrency_level in range(1, max(max(model_info['concurrency_metric']) for experiment in config['experiments'] for model_info in experiment['model_list']) + 1):\n",
    "            tasks = []\n",
    "            for experiment in config['experiments']:\n",
    "                for model_info in experiment['model_list']:\n",
    "                    if concurrency_level in model_info['concurrency_metric']:\n",
    "                        task = asyncio.create_task(process_model(model_info, all_payloads, csv_writer, concurrency_level))\n",
    "                        time.sleep(2)\n",
    "                        tasks.append(task)\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "csv_file_path = 'async_bedrock_model_performance.csv'\n",
    "init_csv_file(csv_file_path)\n",
    "\n",
    "# Create payloads\n",
    "all_payloads = create_payloads_for_all_models(transcript_files, config)\n",
    "\n",
    "# Run the event loop and get results\n",
    "loop = asyncio.get_event_loop()\n",
    "if loop.is_running():\n",
    "    task = asyncio.ensure_future(process_all_models(all_payloads, config, csv_file_path))\n",
    "else:\n",
    "    loop.run_until_complete(process_all_models(all_payloads, config, csv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
