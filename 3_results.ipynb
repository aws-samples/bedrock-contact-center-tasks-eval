{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import yaml\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from botocore.config import Config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting boto3\n",
      "  Using cached boto3-1.34.13-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.13 (from boto3)\n",
      "  Using cached botocore-1.34.13-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Using cached s3transfer-0.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.35.0,>=1.34.13->boto3)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore<1.35.0,>=1.34.13->boto3)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.13->boto3)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached boto3-1.34.13-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.34.13-py3-none-any.whl (11.9 MB)\n",
      "Using cached s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: urllib3, six, jmespath, python-dateutil, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: six\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: jmespath\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: botocore 1.34.13\n",
      "    Uninstalling botocore-1.34.13:\n",
      "      Successfully uninstalled botocore-1.34.13\n",
      "  Attempting uninstall: s3transfer\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: s3transfer 0.10.0\n",
      "    Uninstalling s3transfer-0.10.0:\n",
      "      Successfully uninstalled s3transfer-0.10.0\n",
      "  Attempting uninstall: boto3\n",
      "\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: boto3 1.34.13\n",
      "    Uninstalling boto3-1.34.13:\n",
      "      Successfully uninstalled boto3-1.34.13\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.30.2 requires botocore==1.32.2, but you have botocore 1.34.13 which is incompatible.\n",
      "awscli 1.30.2 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.10.0 which is incompatible.\n",
      "trulens-eval 0.20.0 requires pydantic<3,>=2, but you have pydantic 1.10.13 which is incompatible.\n",
      "sagemaker 2.203.0 requires urllib3<1.27, but you have urllib3 2.0.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.34.13 botocore-1.34.13 jmespath-1.0.1 python-dateutil-2.8.2 s3transfer-0.10.0 six-1.16.0 urllib3-2.0.7\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install boto3 --force-reinstall\n",
    "# !pip install botocore --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: botocore in /opt/homebrew/lib/python3.11/site-packages (1.34.13)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from botocore) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/homebrew/lib/python3.11/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/homebrew/lib/python3.11/site-packages (from botocore) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/platformdirs-4.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/filelock-3.13.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/distlib-0.3.8.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s,%(module)s,%(processName)s,%(levelname)s,%(message)s', level=logging.INFO, stream=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:12:27,654,2625127137,MainProcess,INFO,config read from config.yaml -> {\n",
      "  \"app_name\": \"contact-center-transcript-summarization\",\n",
      "  \"aws\": {\n",
      "    \"region\": \"us-east-1\",\n",
      "    \"sagemaker_execution_role\": \"Admin\"\n",
      "  },\n",
      "  \"dir\": {\n",
      "    \"data\": \"data\",\n",
      "    \"raw\": \"data/raw\",\n",
      "    \"golden\": \"data/raw/golden\",\n",
      "    \"prompts\": \"data/prompts\",\n",
      "    \"models\": \"data/models\",\n",
      "    \"metrics\": \"data/metrics\",\n",
      "    \"completions\": \"data/completions\"\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"raw_data_file\": \"data.csv\",\n",
      "    \"golden_transcript\": \"data/raw/golden/transcript.txt\",\n",
      "    \"golden_transcript_summary\": \"data/raw/golden/summary.txt\"\n",
      "  },\n",
      "  \"prompt\": {\n",
      "    \"very_large_prompt\": {\n",
      "      \"sleep_time\": 180,\n",
      "      \"threshold\": 70000\n",
      "    },\n",
      "    \"normal_prompt\": {\n",
      "      \"sleep_time\": 60\n",
      "    }\n",
      "  },\n",
      "  \"max_retries\": 3,\n",
      "  \"desired_word_count_for_summary\": 80,\n",
      "  \"experiments\": [\n",
      "    {\n",
      "      \"name\": \"single-line-reason\",\n",
      "      \"prompt_template\": null,\n",
      "      \"reps\": 3,\n",
      "      \"model_list\": [\n",
      "        {\n",
      "          \"model\": \"anthropic.claude-instant-v1\",\n",
      "          \"prompt_template\": \"anthropic_template_single_line_reason.txt\"\n",
      "        },\n",
      "        {\n",
      "          \"model\": \"amazon.titan-text-express-v1\",\n",
      "          \"prompt_template\": \"titan_template.txt\"\n",
      "        },\n",
      "        {\n",
      "          \"model\": \"cohere.command-text-v14\",\n",
      "          \"prompt_template\": \"cohere_template.txt\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"bedrock_models\": {\n",
      "    \"cohere.command-text-v14\": {\n",
      "      \"context_length\": 4000,\n",
      "      \"prompt_token_pricing_per_million\": 1.5,\n",
      "      \"completion_token_pricing_per_million\": 2.0,\n",
      "      \"inference_param_set\": \"cohere\"\n",
      "    },\n",
      "    \"amazon.titan-text-express-v1\": {\n",
      "      \"context_length\": 8000,\n",
      "      \"prompt_token_pricing_per_million\": 0.8,\n",
      "      \"completion_token_pricing_per_million\": 1.6,\n",
      "      \"inference_param_set\": \"titan\"\n",
      "    },\n",
      "    \"anthropic.claude-v2:1\": {\n",
      "      \"context_length\": 100000,\n",
      "      \"prompt_token_pricing_per_million\": 8,\n",
      "      \"completion_token_pricing_per_million\": 24,\n",
      "      \"inference_param_set\": \"claude\"\n",
      "    },\n",
      "    \"anthropic.claude-instant-v1\": {\n",
      "      \"context_length\": 9000,\n",
      "      \"prompt_token_pricing_per_million\": 1.63,\n",
      "      \"completion_token_pricing_per_million\": 5.51,\n",
      "      \"inference_param_set\": \"claude\"\n",
      "    }\n",
      "  },\n",
      "  \"inference_params\": {\n",
      "    \"claude\": {\n",
      "      \"max_tokens_to_sample\": 512,\n",
      "      \"temperature\": 0.1,\n",
      "      \"top_k\": 250,\n",
      "      \"top_p\": 0.5,\n",
      "      \"stop_sequences\": [],\n",
      "      \"anthropic_version\": \"bedrock-2023-05-31\"\n",
      "    },\n",
      "    \"titan\": {\n",
      "      \"maxTokenCount\": 512,\n",
      "      \"stopSequences\": [],\n",
      "      \"temperature\": 0.1,\n",
      "      \"topP\": 0.9\n",
      "    },\n",
      "    \"cohere\": {\n",
      "      \"max_tokens\": 512,\n",
      "      \"temperature\": 0.1,\n",
      "      \"p\": 0.9,\n",
      "      \"k\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:12:27,661,1113939681,MainProcess,INFO,there are 45 files in data/metrics/**/*/*.json\n"
     ]
    }
   ],
   "source": [
    "fpath = os.path.join(config['dir']['metrics'], \"**\", \"*\", \"*.json\")\n",
    "file_list = glob.glob(fpath)\n",
    "logger.info(f\"there are {len(file_list)} files in {fpath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:12:27,676,2271422989,MainProcess,INFO,all metrics data is read into a dataframe of shape (45, 11)\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "for f in file_list:\n",
    "    transcript_id = \"_\".join(os.path.basename(f).split('_')[:-1])\n",
    "    metrics.append(json.loads(Path(f).read_text()) | dict(transcript_id=transcript_id))\n",
    "df = pd.DataFrame(metrics)\n",
    "logger.info(f\"all metrics data is read into a dataframe of shape {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wc_gt_than_desired'] = df.completion_word_count > config['desired_word_count_for_summary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>model_id</th>\n",
       "      <th>time_taken_in_seconds</th>\n",
       "      <th>completion_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>cost</th>\n",
       "      <th>completion_word_count</th>\n",
       "      <th>experiment</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>wc_gt_than_desired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>You are an AI bot that is good at determining ...</td>\n",
       "      <td>&lt;output&gt;\\nHere are the action items I gathered...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>5.222832</td>\n",
       "      <td>167</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>122</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_amazon.titan-text-exp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>You are an AI bot that is good at determining ...</td>\n",
       "      <td>&lt;output&gt;\\nHere are the action items I gathered...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>5.175840</td>\n",
       "      <td>167</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>122</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_amazon.titan-text-exp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>You are an AI bot that is good at determining ...</td>\n",
       "      <td>&lt;output&gt;\\nA: \\n- Research mobile gaming trends...</td>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>7.171072</td>\n",
       "      <td>220</td>\n",
       "      <td>788</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>165</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_cohere.command-text-v14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Human: Read the conversation between the call ...</td>\n",
       "      <td>&lt;output&gt;\\nA: \\n- Research recent trends and gr...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>1.307436</td>\n",
       "      <td>110</td>\n",
       "      <td>775</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>72</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_anthropic.claude-inst...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Human: Read the conversation between the call ...</td>\n",
       "      <td>&lt;output&gt;\\nA: \\n- Research recent trends and gr...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>1.199874</td>\n",
       "      <td>99</td>\n",
       "      <td>775</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>63</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_anthropic.claude-inst...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exception                                             prompt  \\\n",
       "0      None  You are an AI bot that is good at determining ...   \n",
       "1      None  You are an AI bot that is good at determining ...   \n",
       "2      None  You are an AI bot that is good at determining ...   \n",
       "3      None  Human: Read the conversation between the call ...   \n",
       "4      None  Human: Read the conversation between the call ...   \n",
       "\n",
       "                                          completion  \\\n",
       "0  <output>\\nHere are the action items I gathered...   \n",
       "1  <output>\\nHere are the action items I gathered...   \n",
       "2  <output>\\nA: \\n- Research mobile gaming trends...   \n",
       "3  <output>\\nA: \\n- Research recent trends and gr...   \n",
       "4  <output>\\nA: \\n- Research recent trends and gr...   \n",
       "\n",
       "                       model_id  time_taken_in_seconds  \\\n",
       "0  amazon.titan-text-express-v1               5.222832   \n",
       "1  amazon.titan-text-express-v1               5.175840   \n",
       "2       cohere.command-text-v14               7.171072   \n",
       "3   anthropic.claude-instant-v1               1.307436   \n",
       "4   anthropic.claude-instant-v1               1.199874   \n",
       "\n",
       "   completion_token_count  prompt_token_count      cost  \\\n",
       "0                     167                 793  0.000902   \n",
       "1                     167                 793  0.000902   \n",
       "2                     220                 788  0.001622   \n",
       "3                     110                 775  0.001869   \n",
       "4                      99                 775  0.001809   \n",
       "\n",
       "   completion_word_count          experiment  \\\n",
       "0                    122  single-line-reason   \n",
       "1                    122  single-line-reason   \n",
       "2                    165  single-line-reason   \n",
       "3                     72  single-line-reason   \n",
       "4                     63  single-line-reason   \n",
       "\n",
       "                                       transcript_id  wc_gt_than_desired  \n",
       "0  call_center_transcript_2_amazon.titan-text-exp...                True  \n",
       "1  call_center_transcript_2_amazon.titan-text-exp...                True  \n",
       "2   call_center_transcript_2_cohere.command-text-v14                True  \n",
       "3  call_center_transcript_2_anthropic.claude-inst...               False  \n",
       "4  call_center_transcript_2_anthropic.claude-inst...               False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wc_gt_than_desired\n",
       "True     24\n",
       "False    21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['wc_gt_than_desired'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.13\n"
     ]
    }
   ],
   "source": [
    "## make sure the boto3 version is up to date \n",
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:12:28,179,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:28,179,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:28,193,credentials,MainProcess,INFO,Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-04 18:12:28,503,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:28,748,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:28,960,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:28,960,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:29,157,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:29,362,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:29,573,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:29,574,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:29,794,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:30,036,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:30,318,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:30,319,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:30,528,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:30,737,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:30,934,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:30,935,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:31,171,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:31,412,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:31,648,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:31,649,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:31,851,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:32,091,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:32,325,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:32,326,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:32,570,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:32,784,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:32,984,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:32,986,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:33,192,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:33,410,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:33,633,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:33,634,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:33,857,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:34,103,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:34,327,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:34,329,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:34,552,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:34,783,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:35,025,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:35,028,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:35,791,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:36,015,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:36,236,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:36,237,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:36,457,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:36,673,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:36,888,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:36,889,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:37,145,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:37,383,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:37,669,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:37,670,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:37,869,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:38,066,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:38,283,3153552436,MainProcess,INFO,there are 3 pairs in this dataframe of shape (3, 12)\n",
      "2024-01-04 18:12:38,283,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:38,554,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n",
      "2024-01-04 18:12:38,790,rouge_scorer,MainProcess,INFO,Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>similarity_metrics</th>\n",
       "      <th>rouge_l_f1_score_within_responses_mean</th>\n",
       "      <th>cosine_similarity_within_responses_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>call_center_transcript_0_amazon.titan-text-exp...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.7942, 0.9739638039604354)</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>0.973964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>call_center_transcript_0_anthropic.claude-inst...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.9286, 0.9978085157422109)</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.997809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>call_center_transcript_0_cohere.command-text-v14</td>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.4117, 0.9544265222292876)</td>\n",
       "      <td>0.411700</td>\n",
       "      <td>0.954427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>call_center_transcript_1_amazon.titan-text-exp...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(1.0, 0.9999999999999997)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>call_center_transcript_1_anthropic.claude-inst...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(1.0, 1.0)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>call_center_transcript_1_cohere.command-text-v14</td>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.40863333333333335, 0.9623230770004018)</td>\n",
       "      <td>0.408633</td>\n",
       "      <td>0.962323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>call_center_transcript_2_amazon.titan-text-exp...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(1.0, 0.9999999999999997)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>call_center_transcript_2_anthropic.claude-inst...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.9576666666666668, 0.9954751847106111)</td>\n",
       "      <td>0.957667</td>\n",
       "      <td>0.995475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>call_center_transcript_2_cohere.command-text-v14</td>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.41923333333333335, 0.9205654164813888)</td>\n",
       "      <td>0.419233</td>\n",
       "      <td>0.920565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>call_center_transcript_3_amazon.titan-text-exp...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.7423333333333334, 0.9228218774015087)</td>\n",
       "      <td>0.742333</td>\n",
       "      <td>0.922822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>call_center_transcript_3_anthropic.claude-inst...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(1.0, 1.0)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>call_center_transcript_3_cohere.command-text-v14</td>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.7333333333333334, 0.9561205339185763)</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.956121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>call_center_transcript_4_amazon.titan-text-exp...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.5343666666666667, 0.9227212368407608)</td>\n",
       "      <td>0.534367</td>\n",
       "      <td>0.922721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>call_center_transcript_4_anthropic.claude-inst...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(1.0, 1.0)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>call_center_transcript_4_cohere.command-text-v14</td>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>(0.43143333333333334, 0.9527937324263033)</td>\n",
       "      <td>0.431433</td>\n",
       "      <td>0.952794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        transcript_id  \\\n",
       "0   call_center_transcript_0_amazon.titan-text-exp...   \n",
       "1   call_center_transcript_0_anthropic.claude-inst...   \n",
       "2    call_center_transcript_0_cohere.command-text-v14   \n",
       "3   call_center_transcript_1_amazon.titan-text-exp...   \n",
       "4   call_center_transcript_1_anthropic.claude-inst...   \n",
       "5    call_center_transcript_1_cohere.command-text-v14   \n",
       "6   call_center_transcript_2_amazon.titan-text-exp...   \n",
       "7   call_center_transcript_2_anthropic.claude-inst...   \n",
       "8    call_center_transcript_2_cohere.command-text-v14   \n",
       "9   call_center_transcript_3_amazon.titan-text-exp...   \n",
       "10  call_center_transcript_3_anthropic.claude-inst...   \n",
       "11   call_center_transcript_3_cohere.command-text-v14   \n",
       "12  call_center_transcript_4_amazon.titan-text-exp...   \n",
       "13  call_center_transcript_4_anthropic.claude-inst...   \n",
       "14   call_center_transcript_4_cohere.command-text-v14   \n",
       "\n",
       "                        model_id          experiment  \\\n",
       "0   amazon.titan-text-express-v1  single-line-reason   \n",
       "1    anthropic.claude-instant-v1  single-line-reason   \n",
       "2        cohere.command-text-v14  single-line-reason   \n",
       "3   amazon.titan-text-express-v1  single-line-reason   \n",
       "4    anthropic.claude-instant-v1  single-line-reason   \n",
       "5        cohere.command-text-v14  single-line-reason   \n",
       "6   amazon.titan-text-express-v1  single-line-reason   \n",
       "7    anthropic.claude-instant-v1  single-line-reason   \n",
       "8        cohere.command-text-v14  single-line-reason   \n",
       "9   amazon.titan-text-express-v1  single-line-reason   \n",
       "10   anthropic.claude-instant-v1  single-line-reason   \n",
       "11       cohere.command-text-v14  single-line-reason   \n",
       "12  amazon.titan-text-express-v1  single-line-reason   \n",
       "13   anthropic.claude-instant-v1  single-line-reason   \n",
       "14       cohere.command-text-v14  single-line-reason   \n",
       "\n",
       "                           similarity_metrics  \\\n",
       "0                (0.7942, 0.9739638039604354)   \n",
       "1                (0.9286, 0.9978085157422109)   \n",
       "2                (0.4117, 0.9544265222292876)   \n",
       "3                   (1.0, 0.9999999999999997)   \n",
       "4                                  (1.0, 1.0)   \n",
       "5   (0.40863333333333335, 0.9623230770004018)   \n",
       "6                   (1.0, 0.9999999999999997)   \n",
       "7    (0.9576666666666668, 0.9954751847106111)   \n",
       "8   (0.41923333333333335, 0.9205654164813888)   \n",
       "9    (0.7423333333333334, 0.9228218774015087)   \n",
       "10                                 (1.0, 1.0)   \n",
       "11   (0.7333333333333334, 0.9561205339185763)   \n",
       "12   (0.5343666666666667, 0.9227212368407608)   \n",
       "13                                 (1.0, 1.0)   \n",
       "14  (0.43143333333333334, 0.9527937324263033)   \n",
       "\n",
       "    rouge_l_f1_score_within_responses_mean  \\\n",
       "0                                 0.794200   \n",
       "1                                 0.928600   \n",
       "2                                 0.411700   \n",
       "3                                 1.000000   \n",
       "4                                 1.000000   \n",
       "5                                 0.408633   \n",
       "6                                 1.000000   \n",
       "7                                 0.957667   \n",
       "8                                 0.419233   \n",
       "9                                 0.742333   \n",
       "10                                1.000000   \n",
       "11                                0.733333   \n",
       "12                                0.534367   \n",
       "13                                1.000000   \n",
       "14                                0.431433   \n",
       "\n",
       "    cosine_similarity_within_responses_mean  \n",
       "0                                  0.973964  \n",
       "1                                  0.997809  \n",
       "2                                  0.954427  \n",
       "3                                  1.000000  \n",
       "4                                  1.000000  \n",
       "5                                  0.962323  \n",
       "6                                  1.000000  \n",
       "7                                  0.995475  \n",
       "8                                  0.920565  \n",
       "9                                  0.922822  \n",
       "10                                 1.000000  \n",
       "11                                 0.956121  \n",
       "12                                 0.922721  \n",
       "13                                 1.000000  \n",
       "14                                 0.952794  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from utils import get_rouge_l_score, get_cosine_similarity\n",
    "import numpy as np\n",
    "def compare_results(df):\n",
    "    pairs = list(combinations(df.completion, 2))\n",
    "    rouge_l_f1scores = []\n",
    "    cosine_similarities = []\n",
    "    logger.info(f\"there are {len(pairs)} pairs in this dataframe of shape {df.shape}\")\n",
    "    for p in pairs:\n",
    "        rouge_l_f1scores.append(get_rouge_l_score(p[0], p[1]))\n",
    "        cosine_similarities.append(get_cosine_similarity(p[0], p[1]))\n",
    "    # print(f\"{np.mean(rouge_l_f1scores)}, {np.mean(cosine_similarities)}\")\n",
    "    return (np.mean(rouge_l_f1scores), np.mean(cosine_similarities))\n",
    "\n",
    "\n",
    "df_per_model_completion_similarity = df.groupby(['transcript_id', 'model_id', 'experiment']).apply(compare_results).reset_index()\n",
    "df_per_model_completion_similarity.columns = ['transcript_id', 'model_id', 'experiment', 'similarity_metrics']\n",
    "df_per_model_completion_similarity['rouge_l_f1_score_within_responses_mean'] = df_per_model_completion_similarity['similarity_metrics'].map(lambda x: x[0])\n",
    "df_per_model_completion_similarity['cosine_similarity_within_responses_mean'] = df_per_model_completion_similarity['similarity_metrics'].map(lambda x: x[1])\n",
    "\n",
    "df_per_model_completion_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_per_model_completion_similarity.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>rouge_l_f1_score_within_responses_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.814180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.977253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.480867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  \\\n",
       "0  amazon.titan-text-express-v1  single-line-reason   \n",
       "1   anthropic.claude-instant-v1  single-line-reason   \n",
       "2       cohere.command-text-v14  single-line-reason   \n",
       "\n",
       "   rouge_l_f1_score_within_responses_mean  \n",
       "0                                0.814180  \n",
       "1                                0.977253  \n",
       "2                                0.480867  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_per_model_rouge_l_f1_score = df_per_model_completion_similarity.groupby(['model_id', 'experiment'])['rouge_l_f1_score_within_responses_mean'].mean().reset_index()\n",
    "df_per_model_rouge_l_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>cosine_similarity_within_responses_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.963901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.998657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.949246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  \\\n",
       "0  amazon.titan-text-express-v1  single-line-reason   \n",
       "1   anthropic.claude-instant-v1  single-line-reason   \n",
       "2       cohere.command-text-v14  single-line-reason   \n",
       "\n",
       "   cosine_similarity_within_responses_mean  \n",
       "0                                 0.963901  \n",
       "1                                 0.998657  \n",
       "2                                 0.949246  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_per_model_cosine_similarity = df_per_model_completion_similarity.groupby(['model_id', 'experiment'])['cosine_similarity_within_responses_mean'].mean().reset_index()\n",
    "df_per_model_cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  count\n",
       "0  amazon.titan-text-express-v1  single-line-reason     15\n",
       "1   anthropic.claude-instant-v1  single-line-reason     15\n",
       "2       cohere.command-text-v14  single-line-reason     15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_counts = df[['model_id', 'experiment']].value_counts().reset_index()\n",
    "df_counts.columns = ['model_id', 'experiment', 'count']\n",
    "df_counts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>count_of_completions_gt_wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  \\\n",
       "0       cohere.command-text-v14  single-line-reason   \n",
       "1  amazon.titan-text-express-v1  single-line-reason   \n",
       "2   anthropic.claude-instant-v1  single-line-reason   \n",
       "\n",
       "   count_of_completions_gt_wc  \n",
       "0                          13  \n",
       "1                           8  \n",
       "2                           3  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pct_gt_wc = df[df.wc_gt_than_desired == True].value_counts(['model_id', 'experiment'], normalize=False).reset_index()\n",
    "df_pct_gt_wc.columns = ['model_id', 'experiment', 'count_of_completions_gt_wc']\n",
    "df_pct_gt_wc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>count_of_completions_gt_wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  \\\n",
       "0       cohere.command-text-v14  single-line-reason   \n",
       "1  amazon.titan-text-express-v1  single-line-reason   \n",
       "2   anthropic.claude-instant-v1  single-line-reason   \n",
       "\n",
       "   count_of_completions_gt_wc  \n",
       "0                          13  \n",
       "1                           8  \n",
       "2                           3  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_to_add = []\n",
    "for r in df[['model_id', 'experiment']].drop_duplicates().iterrows():\n",
    "    model_id = r[1]['model_id']\n",
    "    experiment = r[1]['experiment']\n",
    "    df_temp = df_pct_gt_wc[(df_pct_gt_wc.model_id == model_id) & (df_pct_gt_wc.experiment == experiment)]\n",
    "    if df_temp.shape[0] == 0:\n",
    "        rows_to_add.append(dict(model_id=model_id, experiment=experiment, count_of_completions_gt_wc=0))\n",
    "rows_to_add\n",
    "df_pct_gt_wc = pd.concat([df_pct_gt_wc, pd.DataFrame(rows_to_add)])\n",
    "df_pct_gt_wc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>model_id</th>\n",
       "      <th>time_taken_in_seconds</th>\n",
       "      <th>completion_token_count</th>\n",
       "      <th>prompt_token_count</th>\n",
       "      <th>cost</th>\n",
       "      <th>completion_word_count</th>\n",
       "      <th>experiment</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>wc_gt_than_desired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>You are an AI bot that is good at determining ...</td>\n",
       "      <td>&lt;output&gt;\\nHere are the action items I gathered...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>5.222832</td>\n",
       "      <td>167</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>122</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_amazon.titan-text-exp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>You are an AI bot that is good at determining ...</td>\n",
       "      <td>&lt;output&gt;\\nHere are the action items I gathered...</td>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>5.175840</td>\n",
       "      <td>167</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>122</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_amazon.titan-text-exp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>You are an AI bot that is good at determining ...</td>\n",
       "      <td>&lt;output&gt;\\nA: \\n- Research mobile gaming trends...</td>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>7.171072</td>\n",
       "      <td>220</td>\n",
       "      <td>788</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>165</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_cohere.command-text-v14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Human: Read the conversation between the call ...</td>\n",
       "      <td>&lt;output&gt;\\nA: \\n- Research recent trends and gr...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>1.307436</td>\n",
       "      <td>110</td>\n",
       "      <td>775</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>72</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_anthropic.claude-inst...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Human: Read the conversation between the call ...</td>\n",
       "      <td>&lt;output&gt;\\nA: \\n- Research recent trends and gr...</td>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>1.199874</td>\n",
       "      <td>99</td>\n",
       "      <td>775</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>63</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>call_center_transcript_2_anthropic.claude-inst...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exception                                             prompt  \\\n",
       "0      None  You are an AI bot that is good at determining ...   \n",
       "1      None  You are an AI bot that is good at determining ...   \n",
       "2      None  You are an AI bot that is good at determining ...   \n",
       "3      None  Human: Read the conversation between the call ...   \n",
       "4      None  Human: Read the conversation between the call ...   \n",
       "\n",
       "                                          completion  \\\n",
       "0  <output>\\nHere are the action items I gathered...   \n",
       "1  <output>\\nHere are the action items I gathered...   \n",
       "2  <output>\\nA: \\n- Research mobile gaming trends...   \n",
       "3  <output>\\nA: \\n- Research recent trends and gr...   \n",
       "4  <output>\\nA: \\n- Research recent trends and gr...   \n",
       "\n",
       "                       model_id  time_taken_in_seconds  \\\n",
       "0  amazon.titan-text-express-v1               5.222832   \n",
       "1  amazon.titan-text-express-v1               5.175840   \n",
       "2       cohere.command-text-v14               7.171072   \n",
       "3   anthropic.claude-instant-v1               1.307436   \n",
       "4   anthropic.claude-instant-v1               1.199874   \n",
       "\n",
       "   completion_token_count  prompt_token_count      cost  \\\n",
       "0                     167                 793  0.000902   \n",
       "1                     167                 793  0.000902   \n",
       "2                     220                 788  0.001622   \n",
       "3                     110                 775  0.001869   \n",
       "4                      99                 775  0.001809   \n",
       "\n",
       "   completion_word_count          experiment  \\\n",
       "0                    122  single-line-reason   \n",
       "1                    122  single-line-reason   \n",
       "2                    165  single-line-reason   \n",
       "3                     72  single-line-reason   \n",
       "4                     63  single-line-reason   \n",
       "\n",
       "                                       transcript_id  wc_gt_than_desired  \n",
       "0  call_center_transcript_2_amazon.titan-text-exp...                True  \n",
       "1  call_center_transcript_2_amazon.titan-text-exp...                True  \n",
       "2   call_center_transcript_2_cohere.command-text-v14                True  \n",
       "3  call_center_transcript_2_anthropic.claude-inst...               False  \n",
       "4  call_center_transcript_2_anthropic.claude-inst...               False  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>cosine_similarity_mean</th>\n",
       "      <th>rouge_l_f1_score_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.963901</td>\n",
       "      <td>0.814180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.998657</td>\n",
       "      <td>0.977253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>0.949246</td>\n",
       "      <td>0.480867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  cosine_similarity_mean  \\\n",
       "0  amazon.titan-text-express-v1  single-line-reason                0.963901   \n",
       "1   anthropic.claude-instant-v1  single-line-reason                0.998657   \n",
       "2       cohere.command-text-v14  single-line-reason                0.949246   \n",
       "\n",
       "   rouge_l_f1_score_mean  \n",
       "0               0.814180  \n",
       "1               0.977253  \n",
       "2               0.480867  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary_mean = df_per_model_completion_similarity.groupby(['model_id', 'experiment']).agg({\n",
    "    'cosine_similarity_within_responses_mean': 'mean', \n",
    "    'rouge_l_f1_score_within_responses_mean': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df_summary_mean.columns = ['model_id', 'experiment', 'cosine_similarity_mean', 'rouge_l_f1_score_mean']\n",
    "\n",
    "df_summary_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>time_taken_in_seconds_q95</th>\n",
       "      <th>completion_token_count_q95</th>\n",
       "      <th>prompt_token_count_q95</th>\n",
       "      <th>completion_word_count_q95</th>\n",
       "      <th>cost_q95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>7.132016</td>\n",
       "      <td>213.9</td>\n",
       "      <td>976.0</td>\n",
       "      <td>149.2</td>\n",
       "      <td>0.001123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>2.573874</td>\n",
       "      <td>122.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.002221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>8.835256</td>\n",
       "      <td>238.6</td>\n",
       "      <td>963.0</td>\n",
       "      <td>179.1</td>\n",
       "      <td>0.001804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  \\\n",
       "0  amazon.titan-text-express-v1  single-line-reason   \n",
       "1   anthropic.claude-instant-v1  single-line-reason   \n",
       "2       cohere.command-text-v14  single-line-reason   \n",
       "\n",
       "   time_taken_in_seconds_q95  completion_token_count_q95  \\\n",
       "0                   7.132016                       213.9   \n",
       "1                   2.573874                       122.0   \n",
       "2                   8.835256                       238.6   \n",
       "\n",
       "   prompt_token_count_q95  completion_word_count_q95  cost_q95  \n",
       "0                   976.0                      149.2  0.001123  \n",
       "1                   950.0                       89.0  0.002221  \n",
       "2                   963.0                      179.1  0.001804  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_summary_q95 = df.groupby(['model_id', 'experiment'])['time_taken_in_seconds', 'completion_token_count', 'prompt_token_count', 'completion_word_count', 'cost'].quantile(0.95).reset_index()\n",
    "# df_summary_q95.columns = ['model_id', 'experiment', 'time_taken_in_seconds_q95', 'completion_token_count_q95', 'prompt_token_count_q95', 'completion_word_count_q95', 'cost_q95']\n",
    "# df_summary_q95 = df.groupby(['model_id', 'experiment'])[['time_taken_in_seconds', 'completion_token_count', 'prompt_token_count', 'completion_word_count', 'cost']].quantile(0.95).reset_index()\n",
    "# df_summary_q95.columns = ['model_id', 'experiment', 'time_taken_in_seconds_q95', 'completion_token_count_q95', 'prompt_token_count_q95', 'completion_word_count_q95', 'cost_q95']\n",
    "\n",
    "\n",
    "# df_summary_q95\n",
    "\n",
    "df_summary_q95 = df.groupby(['model_id', 'experiment'])[['time_taken_in_seconds', 'completion_token_count', 'prompt_token_count', 'completion_word_count', 'cost']].quantile(0.95).reset_index()\n",
    "\n",
    "# Renaming columns\n",
    "df_summary_q95.columns = ['model_id', 'experiment', 'time_taken_in_seconds_q95', 'completion_token_count_q95', 'prompt_token_count_q95', 'completion_word_count_q95', 'cost_q95']\n",
    "\n",
    "df_summary_q95\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>count_of_completions_gt_wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  \\\n",
       "0       cohere.command-text-v14  single-line-reason   \n",
       "1  amazon.titan-text-express-v1  single-line-reason   \n",
       "2   anthropic.claude-instant-v1  single-line-reason   \n",
       "\n",
       "   count_of_completions_gt_wc  \n",
       "0                          13  \n",
       "1                           8  \n",
       "2                           3  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pct_gt_wc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 9)\n",
      "(3, 10)\n",
      "(3, 11)\n",
      "(3, 12)\n",
      "(3, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 13)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_summary_all = pd.merge(df_summary_q95, df_summary_mean)\n",
    "print(df_summary_all.shape)\n",
    "df_summary_all = pd.merge(df_summary_all, df_counts)\n",
    "print(df_summary_all.shape)\n",
    "\n",
    "df_summary_all = pd.merge(df_summary_all, df_pct_gt_wc)\n",
    "print(df_summary_all.shape)\n",
    "\n",
    "df_summary_all = pd.merge(df_summary_all, df_per_model_cosine_similarity)\n",
    "print(df_summary_all.shape)\n",
    "\n",
    "df_summary_all = pd.merge(df_summary_all, df_per_model_rouge_l_f1_score)\n",
    "print(df_summary_all.shape)\n",
    "\n",
    "df_summary_all.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>time_taken_in_seconds_q95</th>\n",
       "      <th>completion_token_count_q95</th>\n",
       "      <th>prompt_token_count_q95</th>\n",
       "      <th>completion_word_count_q95</th>\n",
       "      <th>cost_q95</th>\n",
       "      <th>cosine_similarity_mean</th>\n",
       "      <th>rouge_l_f1_score_mean</th>\n",
       "      <th>count</th>\n",
       "      <th>cosine_similarity_within_responses_mean</th>\n",
       "      <th>rouge_l_f1_score_within_responses_mean</th>\n",
       "      <th>percent_of_completions_gt_wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon.titan-text-express-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>7</td>\n",
       "      <td>214</td>\n",
       "      <td>976</td>\n",
       "      <td>149</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.963901</td>\n",
       "      <td>0.814180</td>\n",
       "      <td>15</td>\n",
       "      <td>0.963901</td>\n",
       "      <td>0.814180</td>\n",
       "      <td>53.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anthropic.claude-instant-v1</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>950</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.998657</td>\n",
       "      <td>0.977253</td>\n",
       "      <td>15</td>\n",
       "      <td>0.998657</td>\n",
       "      <td>0.977253</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cohere.command-text-v14</td>\n",
       "      <td>single-line-reason</td>\n",
       "      <td>9</td>\n",
       "      <td>239</td>\n",
       "      <td>963</td>\n",
       "      <td>179</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.949246</td>\n",
       "      <td>0.480867</td>\n",
       "      <td>15</td>\n",
       "      <td>0.949246</td>\n",
       "      <td>0.480867</td>\n",
       "      <td>86.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id          experiment  \\\n",
       "0  amazon.titan-text-express-v1  single-line-reason   \n",
       "1   anthropic.claude-instant-v1  single-line-reason   \n",
       "2       cohere.command-text-v14  single-line-reason   \n",
       "\n",
       "   time_taken_in_seconds_q95  completion_token_count_q95  \\\n",
       "0                          7                         214   \n",
       "1                          3                         122   \n",
       "2                          9                         239   \n",
       "\n",
       "   prompt_token_count_q95  completion_word_count_q95  cost_q95  \\\n",
       "0                     976                        149    0.0011   \n",
       "1                     950                         89    0.0022   \n",
       "2                     963                        179    0.0018   \n",
       "\n",
       "   cosine_similarity_mean  rouge_l_f1_score_mean  count  \\\n",
       "0                0.963901               0.814180     15   \n",
       "1                0.998657               0.977253     15   \n",
       "2                0.949246               0.480867     15   \n",
       "\n",
       "   cosine_similarity_within_responses_mean  \\\n",
       "0                                 0.963901   \n",
       "1                                 0.998657   \n",
       "2                                 0.949246   \n",
       "\n",
       "   rouge_l_f1_score_within_responses_mean  percent_of_completions_gt_wc  \n",
       "0                                0.814180                         53.33  \n",
       "1                                0.977253                         20.00  \n",
       "2                                0.480867                         86.67  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_summary_all = pd.merge(df_summary_q95, df_summary_mean)\n",
    "df_summary_all = pd.merge(df_summary_all, df_counts)\n",
    "df_summary_all = pd.merge(df_summary_all, df_pct_gt_wc)\n",
    "df_summary_all = pd.merge(df_summary_all, df_per_model_cosine_similarity)\n",
    "df_summary_all = pd.merge(df_summary_all, df_per_model_rouge_l_f1_score)\n",
    "df_summary_all.time_taken_in_seconds_q95 = np.round(df_summary_all.time_taken_in_seconds_q95).astype(int)\n",
    "df_summary_all.prompt_token_count_q95 = np.round(df_summary_all.prompt_token_count_q95).astype(int)\n",
    "df_summary_all.completion_token_count_q95 = np.round(df_summary_all.completion_token_count_q95).astype(int)\n",
    "df_summary_all.completion_word_count_q95 = np.round(df_summary_all.completion_word_count_q95).astype(int)\n",
    "df_summary_all.cost_q95 = np.round(df_summary_all.cost_q95, 4)\n",
    "df_summary_all['percent_of_completions_gt_wc'] = np.round((df_summary_all['count_of_completions_gt_wc']/df_summary_all['count'])*100, 2)\n",
    "df_summary_all.drop(['count_of_completions_gt_wc'], axis=1, inplace=True)\n",
    "\n",
    "#, df_counts\n",
    "df_summary_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(config['dir']['metrics'], \"model_metrics.csv\")\n",
    "df_summary_all.to_csv(fpath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ground truth for each file transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_ground_truths(transcripts_dir):\n",
    "    ground_truths = {}\n",
    "    labels_file = os.path.join(transcripts_dir, \"labels.txt\")\n",
    "    if os.path.exists(labels_file):\n",
    "        with open(labels_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) == 2:\n",
    "                    transcript_name, ground_truth = parts[0].strip(), parts[1].strip()\n",
    "                    ground_truths[transcript_name] = ground_truth\n",
    "    return ground_truths\n",
    "\n",
    "def generate_csv(metrics_dir, completions_dir, transcripts_dir, output_csv):\n",
    "    data = []\n",
    "    ground_truths = read_ground_truths(transcripts_dir)\n",
    "\n",
    "    metrics_reason_dir = os.path.join(metrics_dir, \"single-line-reason\")\n",
    "    completions_reason_dir = os.path.join(completions_dir, \"single-line-reason\")\n",
    "    transcripts_reason_dir = transcripts_dir\n",
    "\n",
    "    # Iterate through each subfolder in the 'single-line-reason' directory within metrics\n",
    "    for folder in os.listdir(metrics_reason_dir):\n",
    "        metrics_subfolder = os.path.join(metrics_reason_dir, folder)\n",
    "        completions_subfolder = os.path.join(completions_reason_dir, folder)\n",
    "\n",
    "        if os.path.isdir(metrics_subfolder) and os.path.isdir(completions_subfolder):\n",
    "            # Iterate through each file in the metrics subfolder\n",
    "            for file in os.listdir(metrics_subfolder):\n",
    "                if file.endswith(\"_rep1.json\"):\n",
    "                    base_name = '_'.join(file.split('_')[0:-2])  # Extract the base name before the model name\n",
    "                    json_file_path = os.path.join(metrics_subfolder, file)\n",
    "                    completion_file = file.replace('.json', '.txt')\n",
    "                    transcript_file = base_name + \".txt\"\n",
    "\n",
    "                    # Reading the transcript data\n",
    "                    try:\n",
    "                        with open(os.path.join(transcripts_reason_dir, transcript_file), 'r') as f:\n",
    "                            transcript = f.read().strip()\n",
    "                    except FileNotFoundError:\n",
    "                        transcript = \"Transcript not found\"\n",
    "\n",
    "                    # Get the ground truth summary\n",
    "                    ground_truth = ground_truths.get(base_name, \"Ground truth not found\")\n",
    "\n",
    "                    # Read the model ID and completion from the JSON file\n",
    "                    try:\n",
    "                        with open(json_file_path, 'r') as json_file:\n",
    "                            json_data = json.load(json_file)\n",
    "                            model_id = json_data.get(\"model_id\", \"\")\n",
    "\n",
    "                        with open(os.path.join(completions_subfolder, completion_file), 'r') as f:\n",
    "                            completion = f.read().strip()\n",
    "\n",
    "                        data.append([base_name, transcript, model_id, completion, ground_truth])\n",
    "                    except FileNotFoundError:\n",
    "                        # File not found\n",
    "                        continue\n",
    "\n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data, columns=['Transcript Name', 'Transcript', 'Model ID', 'Completion', 'Ground Truth'])\n",
    "\n",
    "    # Pivot the DataFrame\n",
    "    df_pivot = df.pivot_table(index=['Transcript Name', 'Transcript', 'Ground Truth'], columns='Model ID', values='Completion', aggfunc='first')\n",
    "\n",
    "    # Reset the index to make 'Transcript Name', 'Transcript', and 'Ground Truth' \n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    df_pivot.fillna('', inplace=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_pivot.to_csv(output_csv, index=False)\n",
    "\n",
    "# Define the directories for metrics, completions, and transcripts\n",
    "metrics_dir = \"data/metrics\"\n",
    "completions_dir = \"data/completions\"\n",
    "transcripts_dir = \"data/raw\"  \n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv = \"model_ground_truth_comparison.csv\"\n",
    "\n",
    "# Generate the CSV file\n",
    "generate_csv(metrics_dir, completions_dir, transcripts_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden summaries and model completions for transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_golden_summary(transcripts_dir, base_name):\n",
    "    # Assuming the golden summary file follows a specific naming pattern\n",
    "    golden_summary_file = f\"{base_name}_golden_summary.txt\"\n",
    "    for subdir in os.listdir(transcripts_dir):\n",
    "        subdir_path = os.path.join(transcripts_dir, subdir)\n",
    "        if os.path.isdir(subdir_path) and golden_summary_file in os.listdir(subdir_path):\n",
    "            with open(os.path.join(subdir_path, golden_summary_file), 'r') as file:\n",
    "                return file.read().strip()\n",
    "    return \"Golden summary not found\"\n",
    "\n",
    "def generate_csv(metrics_dir, completions_dir, transcripts_dir, output_csv):\n",
    "    data = []\n",
    "\n",
    "    metrics_reason_dir = os.path.join(metrics_dir, \"single-line-reason\")\n",
    "    completions_reason_dir = os.path.join(completions_dir, \"single-line-reason\")\n",
    "    transcripts_reason_dir = transcripts_dir\n",
    "\n",
    "    # Iterate through each subfolder in the 'single-line-reason' directory within metrics\n",
    "    for folder in os.listdir(metrics_reason_dir):\n",
    "        metrics_subfolder = os.path.join(metrics_reason_dir, folder)\n",
    "        completions_subfolder = os.path.join(completions_reason_dir, folder)\n",
    "\n",
    "        if os.path.isdir(metrics_subfolder) and os.path.isdir(completions_subfolder):\n",
    "            # Iterate through each file in the metrics subfolder\n",
    "            for file in os.listdir(metrics_subfolder):\n",
    "                if file.endswith(\"_rep1.json\"):\n",
    "                    base_name = '_'.join(file.split('_')[0:-2])  # Extract the base name before the model name\n",
    "                    json_file_path = os.path.join(metrics_subfolder, file)\n",
    "                    completion_file = file.replace('.json', '.txt')\n",
    "                    transcript_file = base_name + \".txt\"\n",
    "\n",
    "                    # Reading the transcript data\n",
    "                    try:\n",
    "                        with open(os.path.join(transcripts_reason_dir, transcript_file), 'r') as f:\n",
    "                            transcript = f.read().strip()\n",
    "                    except FileNotFoundError:\n",
    "                        transcript = \"Transcript not found\"\n",
    "\n",
    "                    # Get the golden summary\n",
    "                    golden_summary = read_golden_summary(transcripts_reason_dir, base_name)\n",
    "\n",
    "                    # Read the model ID and completion from the JSON file\n",
    "                    try:\n",
    "                        with open(json_file_path, 'r') as json_file:\n",
    "                            json_data = json.load(json_file)\n",
    "                            model_id = json_data.get(\"model_id\", \"\")\n",
    "\n",
    "                        with open(os.path.join(completions_subfolder, completion_file), 'r') as f:\n",
    "                            completion = f.read().strip()\n",
    "\n",
    "                        data.append([base_name, transcript, model_id, completion, golden_summary])\n",
    "                    except FileNotFoundError:\n",
    "                        # File not found\n",
    "                        continue\n",
    "\n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data, columns=['Transcript Name', 'Transcript', 'Model ID', 'Completion', 'Golden Summary'])\n",
    "\n",
    "    # Pivot the DataFrame\n",
    "    df_pivot = df.pivot_table(index=['Transcript Name', 'Transcript', 'Golden Summary'], columns='Model ID', values='Completion', aggfunc='first')\n",
    "\n",
    "    # Reset the index to make 'Transcript Name', 'Transcript', and 'Golden Summary' regular columns\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    df_pivot.fillna('', inplace=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_pivot.to_csv(output_csv, index=False)\n",
    "\n",
    "# Define the directories for metrics, completions, and transcripts\n",
    "metrics_dir = \"data/metrics\"\n",
    "completions_dir = \"data/completions\"\n",
    "transcripts_dir = \"data/raw\"\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv = \"golden_summary_model_comparison.csv\"\n",
    "\n",
    "# Generate the CSV file\n",
    "generate_csv(metrics_dir, completions_dir, transcripts_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
